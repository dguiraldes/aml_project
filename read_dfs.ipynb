{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process large csv, transform to parquet, re-process, upload to Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Battery Provider data (t_msb1m.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'raw_data/t_msb1m.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataframe in chunks\n",
    "chunksize = 10 ** 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function convert time data to appropriate format and add columns for year, month, day, hour, and minute\n",
    "def handle_time_data(df):\n",
    "    tdf=df['ts'].str.split('+', expand=True)\n",
    "    tdf.columns=['ts','tz']\n",
    "    tdf['tz']='UTC+'+tdf['tz']+':00'\n",
    "    tdf['ts']=pd.to_datetime(tdf['ts'])\n",
    "    tdf['year']=tdf['ts'].dt.year\n",
    "    tdf['month']=tdf['ts'].dt.month\n",
    "    tdf['day']=tdf['ts'].dt.day\n",
    "    tdf['hour']=tdf['ts'].dt.hour\n",
    "    tdf['minute']=tdf['ts'].dt.minute\n",
    "    return tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataframe in chunks\n",
    "chunks = []\n",
    "for chunk in pd.read_csv(csv_file_path, chunksize=chunksize):\n",
    "    tdf = handle_time_data(chunk)\n",
    "    chunk.drop(columns=['ts'], inplace=True)\n",
    "    processed_chunk = pd.concat([tdf, chunk], axis=1)\n",
    "    chunks.append(processed_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the chunks\n",
    "df = pd.concat(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting it to partitioned parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrame to PyArrow Table\n",
    "table = pa.Table.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define partitioning columns (you can choose one or more columns)\n",
    "partition_columns = ['site','year','month','day']\n",
    "\n",
    "# Write PyArrow Table to Parquet with partitioning\n",
    "pq.write_to_dataset(\n",
    "    table,\n",
    "    root_path='processed_data/t_msb1m',\n",
    "    partition_cols=partition_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing parquet file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = pd.read_parquet('processed_data/t_msb1m', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp.sort_values(by=['site','year','month','day','hour','minute'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group according to resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp['min'] = (dfp['minute']//resolution)*resolution\n",
    "dfp.drop(columns=['minute'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp.drop(columns=['ts'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cols = ['discharge_wh', 'charge_wh', 'production_wh',\n",
    "       'consumption_wh', 'gridexport_wh', 'gridimport_wh', 'pvcharge_wh',\n",
    "       'pvcons_wh', 'pvexport_wh', 'griddischarge_wh', 'gridcharge_wh',\n",
    "       'gridcons_wh', 'consdischarge_wh', 'mismatch_wh']\n",
    "\n",
    "last_cols = ['tz','soc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_dict = {col:'last' for col in last_cols}\n",
    "agg_dict.update({col:'mean' for col in mean_cols})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vt/5wzq72vd2mj_qqq_fwp77h_40000gq/T/ipykernel_1315/1446319703.py:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df_grouped = dfp.groupby(['site','year','month','day','hour','min']).agg(agg_dict).reset_index()\n"
     ]
    }
   ],
   "source": [
    "df_grouped = dfp.groupby(['site','year','month','day','hour','min']).agg(agg_dict).reset_index()\n",
    "for col in mean_cols:\n",
    "    df_grouped[col] = df_grouped[col]*resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_grouped.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep data  from 2019-03 to 2020-08\n",
    "initial_moy = 2019*12+3\n",
    "final_moy = 2020*12+8\n",
    "\n",
    "df_filtered['moy'] = df_filtered['year'].astype(int)*12 + df_filtered['month'].astype(int)\n",
    "df_filtered = df_filtered[(df_filtered['moy']>=initial_moy) & (df_filtered['moy']<=final_moy)]\n",
    "\n",
    "df_filtered.drop(columns=['moy'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vt/5wzq72vd2mj_qqq_fwp77h_40000gq/T/ipykernel_1315/4039914746.py:3: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  data_count = df_filtered.dropna().groupby('site').agg({'discharge_wh':'count'})\n"
     ]
    }
   ],
   "source": [
    "# keep only sites with more than 38000 data points (the maximum is 38100) \n",
    "# we lose 16 sited with this filter\n",
    "data_count = df_filtered.dropna().groupby('site').agg({'discharge_wh':'count'})\n",
    "data_count = data_count[data_count['discharge_wh']>=38000]\n",
    "df_filtered = df_filtered[df_filtered['site'].isin(data_count.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select sites with enough additional data \n",
    "df_sites = pd.read_csv('processed_data/merged_meta_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered[df_filtered['site'].isin(df_sites['newsite'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload processed data to Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Accessing credentials\n",
    "db_host = os.getenv(\"DB_HOST\")\n",
    "db_name = os.getenv(\"DB_NAME\")\n",
    "db_user = os.getenv(\"DB_USER\")\n",
    "db_password = os.getenv(\"DB_PASSWORD\")\n",
    "db_port = os.getenv(\"DB_PORT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "conn = psycopg2.connect(\n",
    "    host=db_host,\n",
    "    dbname=db_name,\n",
    "    user=db_user,\n",
    "    password=db_password,\n",
    "    port=db_port\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2.extras as extras \n",
    "\n",
    "def upload_dataframe(conn, df, table): \n",
    "\n",
    "\ttuples = [tuple(x) for x in df.to_numpy()] \n",
    "\n",
    "\tcols = ','.join(list(df.columns)) \n",
    "\t# SQL query to execute \n",
    "\tquery = \"INSERT INTO %s(%s) VALUES %%s\" % (table, cols) \n",
    "\tcursor = conn.cursor() \n",
    "\ttry: \n",
    "\t\textras.execute_values(cursor, query, tuples) \n",
    "\t\tconn.commit() \n",
    "\texcept (Exception, psycopg2.DatabaseError) as error: \n",
    "\t\tprint(\"Error: %s\" % error) \n",
    "\t\tconn.rollback() \n",
    "\t\tcursor.close() \n",
    "\t\treturn 1\n",
    "\tprint(\"the dataframe is inserted\") \n",
    "\tcursor.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_cols =['site','year','month','day']\n",
    "for col in change_cols:\n",
    "    df_filtered[col] = df_filtered[col].astype(int)\n",
    "\n",
    "df_filtered.sort_values(by=['site','year','month','day','hour','min'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dguiraldes/Documents/MSc UCL/Term 2/Advanced Machine Learning/aml_project/amlenv/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# read and upload file by chunks\n",
    "nchunks = 10\n",
    "table = \"agg.t_msb1m\"\n",
    "\n",
    "#split the dataframe into chunks\n",
    "chunks = np.array_split(df_filtered, nchunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 0\n",
      "the dataframe is inserted\n",
      "Processing chunk 1\n",
      "the dataframe is inserted\n",
      "Processing chunk 2\n",
      "the dataframe is inserted\n",
      "Processing chunk 3\n",
      "the dataframe is inserted\n",
      "Processing chunk 4\n",
      "the dataframe is inserted\n",
      "Processing chunk 5\n",
      "the dataframe is inserted\n",
      "Processing chunk 6\n",
      "the dataframe is inserted\n",
      "Processing chunk 7\n",
      "the dataframe is inserted\n",
      "Processing chunk 8\n",
      "the dataframe is inserted\n",
      "Processing chunk 9\n",
      "the dataframe is inserted\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for chunk in chunks:\n",
    "    print(f\"Processing chunk {i}\")\n",
    "    upload_dataframe(conn, chunk, table)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
