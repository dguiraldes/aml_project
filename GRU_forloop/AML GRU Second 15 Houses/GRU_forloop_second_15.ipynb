{"cells":[{"cell_type":"markdown","metadata":{"id":"VcgpitV9OfHB"},"source":["# <center><u> Final LSTM/GRU Layout - Adri and Elian"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20711,"status":"ok","timestamp":1712926828422,"user":{"displayName":"Elian Tago","userId":"06354967366807767203"},"user_tz":-60},"id":"4-CG_TknOmNn","outputId":"ab273db9-0801-4ded-f198-3f2064a0ba2d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting keras-tuner\n","  Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m825.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n","Collecting kt-legacy (from keras-tuner)\n","  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.2.2)\n","Installing collected packages: kt-legacy, keras-tuner\n","Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"]}],"source":["!pip install keras-tuner"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7817,"status":"ok","timestamp":1712926897074,"user":{"displayName":"Elian Tago","userId":"06354967366807767203"},"user_tz":-60},"id":"9A95mRPcQcfe","outputId":"f7546ecb-4ffe-42cb-e5ec-2b8ab3e9e801"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting python-dotenv\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Installing collected packages: python-dotenv\n","Successfully installed python-dotenv-1.0.1\n"]}],"source":["!pip install python-dotenv"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7828,"status":"ok","timestamp":1712926904899,"user":{"displayName":"Elian Tago","userId":"06354967366807767203"},"user_tz":-60},"id":"6u-NdpJnQr4N","outputId":"d6e8fb8b-760f-4fc9-8015-6015dadb22b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: psycopg2 in /usr/local/lib/python3.10/dist-packages (2.9.9)\n"]}],"source":["!pip install psycopg2"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":249,"status":"ok","timestamp":1712927052311,"user":{"displayName":"Elian Tago","userId":"06354967366807767203"},"user_tz":-60},"id":"PRgCkMfKXfZj","outputId":"6a5df128-e279-4433-87b2-d9a010ee6564"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/AML GRU Second 15 Houses\n"]}],"source":["%cd '/content/drive/MyDrive/AML GRU Second 15 Houses'"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":137587,"status":"ok","timestamp":1712927042482,"user":{"displayName":"Elian Tago","userId":"06354967366807767203"},"user_tz":-60},"id":"9Xdmw2Ele0EO","outputId":"4690a3ce-1f71-4c5f-c998-58f7b338d999"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"qwZJc2fzOfHC"},"source":["# Define Inputs"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":210,"status":"ok","timestamp":1712927059761,"user":{"displayName":"Elian Tago","userId":"06354967366807767203"},"user_tz":-60},"id":"WbfKxWtbOfHC"},"outputs":[],"source":["n_input = 144\n","n_features = 5\n","n_output = 72\n","n_split = 2 * 24 * 324"]},{"cell_type":"markdown","metadata":{"id":"v8_y-NfSOfHD"},"source":["# Get Household Data"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":725,"status":"ok","timestamp":1712927063038,"user":{"displayName":"Elian Tago","userId":"06354967366807767203"},"user_tz":-60},"id":"9k9ONj-HOfHD"},"outputs":[],"source":["#Import Libraries and Packages\n","import os\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import psycopg2\n","from dotenv import load_dotenv\n","load_dotenv()\n","\n","def get_household_data(household):\n","    #Access Credentials\n","    # db_host = os.getenv(\"DB_HOST\")\n","    # db_name = os.getenv(\"DB_NAME\")\n","    # db_user = os.getenv(\"DB_USER\")\n","    # db_password = os.getenv(\"DB_PASSWORD\")\n","    # db_port = os.getenv(\"DB_PORT\")\n","\n","    db_host=\"aml-esda-db.postgres.database.azure.com\"\n","    db_name=\"postgres\"\n","    db_user=\"elian\"\n","    db_password=\"2xKuZG~oP2v9\"\n","    db_port=\"5432\"\n","\n","    conn = psycopg2.connect(\n","    host=db_host,\n","    dbname=db_name,\n","    user=db_user,\n","    password=db_password,\n","    port=db_port\n","    )\n","\n","    query=f\"\"\"\n","    select *\n","    from agg.tidy_data_final\n","    where site = {household}\n","    \"\"\"\n","\n","    df = pd.read_sql_query(query, conn)\n","    df = df.set_index('timestamp', drop=False)\n","    df = df.sort_index()\n","\n","    return df\n"]},{"cell_type":"markdown","metadata":{"id":"2HcptxeBOfHD"},"source":["# Prepare Data Set for LSTM/GRU Models"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":1077,"status":"ok","timestamp":1712927065468,"user":{"displayName":"Elian Tago","userId":"06354967366807767203"},"user_tz":-60},"id":"pyKJbBn-OfHD"},"outputs":[],"source":["from sklearn.preprocessing import MaxAbsScaler\n","scaler = MaxAbsScaler()\n","\n","def data_prep(df):\n","    global n_input\n","    global n_features\n","    global n_output\n","    global n_split\n","\n","    # Select features of use for later on\n","    df_input = df[['net_load', 'precipitation_probability',\n","                'solar_radiation','sunshine_duration','weekend_or_bank_holiday',\n","                    'month', 'day', 'hour', 'day_of_week', 'season',\n","                    'avg_net_load',]]\n","\n","\n","    variables = ['net_load','solar_radiation','sunshine_duration', 'precipitation_probability', 'avg_net_load' ]\n","\n","\n","    for var in variables:\n","        new_var_name = var + '_norm'\n","        df_input[new_var_name] = scaler.fit_transform(df_input[[var]])\n","\n","    df_input['sunshine_duration_norm(t+48)'] = df_input['sunshine_duration_norm'].shift(-48) #Add prediction for time the day after\n","    df_input['solar_radiation_norm(t+48)'] = df_input['solar_radiation_norm'].shift(-48) #Add prediction for time the day after\n","    df_input['precipitation_probability_norm(t+48)'] = df_input['precipitation_probability_norm'].shift(-48) #Add prediction for time the day after\n","\n","    df_input.dropna(inplace=True)\n","\n","    # df_input = df_input[['net_load_norm', 'avg_net_load_norm', 'weekend_or_bank_holiday', 'sunshine_duration_norm(t+48)',\n","    #            'solar_radiation_norm(t+48)','precipitation_probability_norm(t+48)',\n","    #            'month', 'day', 'hour', 'day_of_week', 'season',\n","    #                 ]]\n","\n","    df_input = df_input[['net_load_norm', 'weekend_or_bank_holiday', 'sunshine_duration_norm(t+48)',\n","            'solar_radiation_norm(t+48)','precipitation_probability_norm(t+48)'\n","                ]]\n","\n","    X, y = [], []\n","    for i in range(len(df_input) - n_input - n_output + 1):\n","        # Select all columns for the input sequence\n","        X.append(df_input.iloc[i:(i + n_input)].values)\n","\n","        y.append(df_input['net_load_norm'].iloc[(i + n_input):(i + n_input + n_output)].values)\n","\n","    X = np.array(X).reshape((len(X), n_input, -1))\n","    y = np.array(y)\n","\n","    X_train = X[:n_split]\n","    y_train = y[:n_split]\n","\n","    X_test = X[n_split:]\n","    y_test = y[n_split:]\n","\n","    return X_train, y_train, X_test, y_test, df_input"]},{"cell_type":"markdown","metadata":{"id":"246oLvGBOfHE"},"source":["# Build and Fit LSTM/ GRU Models"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":2965,"status":"ok","timestamp":1712927069291,"user":{"displayName":"Elian Tago","userId":"06354967366807767203"},"user_tz":-60},"id":"cWYqnu9FOfHE"},"outputs":[],"source":["# Import all Necessary Keras Libraries\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.losses import MeanSquaredError\n","from tensorflow.keras.losses import MeanAbsoluteError\n","from tensorflow.keras.metrics import RootMeanSquaredError\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n","from tensorflow.keras.models import load_model\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.model_selection import train_test_split\n","\n","def build_model(X_train, y_train, model_name):\n","    model = Sequential()\n","\n","    model.add(GRU(64, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True ))\n","    # model.add(BatchNormalization())\n","    model.add(LeakyReLU(alpha=0.01))\n","\n","    model.add(GRU(32))\n","    # model.add(BatchNormalization())\n","    model.add(LeakyReLU(alpha=0.01))\n","\n","    model.add(Dropout(0.2))\n","    model.add(Dense(y_train.shape[1]))\n","\n","    optimizer = Adam(learning_rate=0.001)\n","\n","    #Compile Model\n","    cp1 = ModelCheckpoint(f'{model_name}.keras', save_best_only=True)\n","    model.compile(loss='mse', optimizer=optimizer,  metrics=[MeanSquaredError(), MeanAbsoluteError()])\n","\n","    #Fit Model to Training Data with 10% Validation Split\n","    history = model.fit(X_train, y_train, epochs=10, batch_size=16, validation_split=0.1, callbacks=[cp1])\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"v2XXvJ6LOfHE"},"source":["# Get Training Outputs"]},{"cell_type":"markdown","metadata":{"id":"lkLljQsIOfHE"},"source":["### Evaluation Metrics"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1712927072400,"user":{"displayName":"Elian Tago","userId":"06354967366807767203"},"user_tz":-60},"id":"T_HT73t6OfHE"},"outputs":[],"source":["def get_train_metrics(model, X_train, y_train):\n","    _, train_mse, train_mae = model.evaluate(X_train, y_train, verbose=0)\n","\n","    return train_mse, train_mae"]},{"cell_type":"markdown","metadata":{"id":"CCqtsdghOfHE"},"source":["### Visualisation"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":181,"status":"ok","timestamp":1712927075982,"user":{"displayName":"Elian Tago","userId":"06354967366807767203"},"user_tz":-60},"id":"8x6A0-DJOfHE"},"outputs":[],"source":["def get_training_viz(start_section, end_section, df_input, X_train, y_train, model):\n","    global n_input\n","    global n_features\n","    global n_output\n","    global n_split\n","\n","\n","\n","    df_ts_train = df_input[n_input:n_split]\n","\n","    X_train_shift = X_train[24:]\n","    y_train_shift = y_train[24:]\n","\n","    X_train_input = X_train_shift[::48]\n","    y_train_input = y_train_shift[::48]\n","\n","\n","    predictions = []\n","\n","    for i in X_train_input[start_section:end_section]:\n","        i_reshaped = i.reshape(1, n_input, n_features)\n","\n","        prediction = model.predict(i_reshaped)\n","\n","        # We want to omit the first 12 hours the predictions i.e., the non-shaded squares in the figure above.\n","        prediction_list = list(prediction[0][24:])\n","        predictions.append(prediction_list)\n","\n","\n","    y_train_input_24h = []\n","\n","    for i in y_train_input[start_section:end_section]:\n","        # i_reshaped = i.reshape(1, n_input, n_features)\n","        y_train_input_24h.append(i[24:])\n","\n","\n","    plt.figure(figsize=(12,4))\n","    plt.title(f\"Date Range: {df_ts_train.index[start_section*48+48].strftime('%d/%m/%Y')} - {df_ts_train.index[end_section*48+48].strftime('%d/%m/%Y')}\")\n","\n","    # We want to plot the predictions from 00:00 to 00:00 the next day.\n","    plt.plot(df_ts_train.index[start_section*48+48: end_section*48+48], np.array(predictions).flatten(), label='Prediction')\n","    plt.plot(df_ts_train.index[start_section*48+48: end_section*48+48], np.array(y_train_input_24h).flatten(), label = 'Actual')\n","    plt.xlabel('Datetime')\n","    plt.ylabel('Normalised Net Load (-)')\n","\n","\n","    # Generate vertical lines for when we predict\n","    xcoords_DAM = df_ts_train.index[start_section*48+72: (end_section)*48+72][::48]\n","\n","    for i, xc in enumerate(xcoords_DAM):\n","        if i == 0:  # First item gets the label\n","            plt.axvline(x=xc, color='red', linestyle='--', alpha=0.5, label='DAM Gate Closure')\n","        else:  # Subsequent items do not\n","            plt.axvline(x=xc, color='red', linestyle='--', alpha=0.5)\n","\n","\n","\n","    # Generate vertical lines for DAM closure\n","    xcoords_pred = df_ts_train.index[start_section*48+48: (end_section+1)*48+48][::48]\n","\n","    for i, xc in enumerate(xcoords_pred):\n","        if i == 0:  # First item gets the label\n","            plt.axvline(x=xc, color='purple', linestyle='--', alpha=0.3, label='Energy Delivery Prediction')\n","        else:  # Subsequent items do not\n","            plt.axvline(x=xc, color='purple', linestyle='--', alpha=0.3)\n","\n","\n","    plt.legend(bbox_to_anchor=(0.5, 1.15), loc='center', borderaxespad=0., ncol=4)"]},{"cell_type":"markdown","metadata":{"id":"YohKZFqAOfHF"},"source":["# Get Test Outputs"]},{"cell_type":"markdown","metadata":{"id":"Je8qn56YOfHF"},"source":["### Evaluation Metrics"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":197,"status":"ok","timestamp":1712927078389,"user":{"displayName":"Elian Tago","userId":"06354967366807767203"},"user_tz":-60},"id":"25LT9VhSOfHF"},"outputs":[],"source":["from sklearn.metrics import mean_squared_error, mean_absolute_error\n","\n","def get_test_metrics(model, X_test, y_test):\n","    global n_input\n","    global n_features\n","    global n_output\n","    global n_split\n","\n","    X_test_shift = X_test[24:]\n","    y_test_shift = y_test[24:]\n","\n","    X_test_input = X_test_shift[::48]\n","    y_test_input = y_test_shift[::48]\n","\n","    predictions = []\n","\n","    for i in X_test_input:\n","        i_reshaped = i.reshape(1, n_input, n_features)\n","\n","        prediction = model.predict(i_reshaped, verbose=0)\n","\n","        # We want to omit the first 12 hours the predictions i.e., the non-shaded squares in the figure above.\n","        prediction_list = list(prediction[0][24:])\n","        predictions.append(prediction_list)\n","\n","\n","    y_test_input_24h = []\n","\n","    for i in y_test_input:\n","        # i_reshaped = i.reshape(1, n_input, n_features)\n","        y_test_input_24h.append(i[24:])\n","\n","    # Flatten the lists of lists into single lists\n","    predictions_flat = [item for sublist in predictions for item in sublist]\n","    y_test_flat = [item for sublist in y_test_input_24h for item in sublist]\n","\n","    # Calculate MSE\n","    test_mse = mean_squared_error(y_test_flat, predictions_flat)\n","\n","    # Calculate MAE\n","    test_mae = mean_absolute_error(y_test_flat, predictions_flat)\n","\n","    return test_mse, test_mae"]},{"cell_type":"markdown","metadata":{"id":"ZU-GeMbhOfHF"},"source":["### Visualisation"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":299,"status":"ok","timestamp":1712927079910,"user":{"displayName":"Elian Tago","userId":"06354967366807767203"},"user_tz":-60},"id":"q7Vbn2OVOfHF"},"outputs":[],"source":["def get_testing_viz(start_section, end_section, df_input, X_test, y_test, model):\n","    global n_input\n","    global n_features\n","    global n_output\n","    global n_split\n","\n","    df_ts_test = df_input[n_input+n_split:]\n","\n","    X_test_shift = X_test[24:]\n","    y_test_shift = y_test[24:]\n","\n","    X_test_input = X_test_shift[::48]\n","    y_test_input = y_test_shift[::48]\n","\n","\n","    predictions = []\n","\n","    for i in X_test_input[start_section:end_section]:\n","        i_reshaped = i.reshape(1, n_input, n_features)\n","\n","        prediction = model.predict(i_reshaped)\n","\n","        # We want to omit the first 12 hours the predictions i.e., the non-shaded squares in the figure above.\n","        prediction_list = list(prediction[0][24:])\n","        predictions.append(prediction_list)\n","\n","\n","    y_test_input_24h = []\n","\n","    for i in y_test_input[start_section:end_section]:\n","        # i_reshaped = i.reshape(1, n_input, n_features)\n","        y_test_input_24h.append(i[24:])\n","\n","\n","    plt.figure(figsize=(12,4))\n","    plt.title(f\"Date Range: {df_ts_test.index[start_section*48+48].strftime('%d/%m/%Y')} - {df_ts_test.index[end_section*48+48].strftime('%d/%m/%Y')}\")\n","\n","    # We want to plot the predictions from 00:00 to 00:00 the next day.\n","    plt.plot(df_ts_test.index[start_section*48+48: end_section*48+48], np.array(predictions).flatten(), label='Prediction')\n","    plt.plot(df_ts_test.index[start_section*48+48: end_section*48+48], np.array(y_test_input_24h).flatten(), label = 'Actual')\n","    plt.xlabel('Datetime')\n","    plt.ylabel('Normalised Net Load (-)')\n","\n","\n","    # Generate vertical lines for when we predict\n","    xcoords_DAM = df_ts_test.index[start_section*48+72: (end_section)*48+72][::48]\n","\n","    for i, xc in enumerate(xcoords_DAM):\n","        if i == 0:  # First item gets the label\n","            plt.axvline(x=xc, color='red', linestyle='--', alpha=0.5, label='DAM Gate Closure')\n","        else:  # Subsequent items do not\n","            plt.axvline(x=xc, color='red', linestyle='--', alpha=0.5)\n","\n","\n","\n","    # Generate vertical lines for DAM closure\n","    xcoords_pred = df_ts_test.index[start_section*48+48: (end_section+1)*48+48][::48]\n","\n","    for i, xc in enumerate(xcoords_pred):\n","        if i == 0:  # First item gets the label\n","            plt.axvline(x=xc, color='purple', linestyle='--', alpha=0.3, label='Energy Delivery Prediction')\n","        else:  # Subsequent items do not\n","            plt.axvline(x=xc, color='purple', linestyle='--', alpha=0.3)\n","\n","\n","    plt.legend(bbox_to_anchor=(0.5, 1.15), loc='center', borderaxespad=0., ncol=4)"]},{"cell_type":"markdown","metadata":{"id":"oLKSTNc4OfHF"},"source":["# Hyperparameter tuning"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":216,"status":"ok","timestamp":1712927081972,"user":{"displayName":"Elian Tago","userId":"06354967366807767203"},"user_tz":-60},"id":"Km7qd8kWOfHF","outputId":"f27cb09a-b9cc-49d7-ab4e-9cc170e3927b"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-14-0c699db5c18e>:6: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n","  from kerastuner.tuners import RandomSearch\n"]}],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, LeakyReLU, GRU\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.metrics import MeanSquaredError, MeanAbsoluteError\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from kerastuner.tuners import RandomSearch\n","\n","def build_model_hyperparm(hp):\n","    model = Sequential()\n","\n","    model.add(GRU(hp.Int('units_1', min_value=32, max_value=256, step=32),\n","                   input_shape=(X_train.shape[1], X_train.shape[2]),\n","                   return_sequences=hp.Choice('return_sequences', [True, False])))\n","    model.add(LeakyReLU(alpha=0.01))\n","\n","    if hp.Choice('return_sequences', [True, False]):\n","        model.add(GRU(hp.Int('units_2', min_value=32, max_value=256, step=32)))\n","        model.add(LeakyReLU(alpha=0.01))\n","\n","    model.add(Dropout(hp.Float('dropout', min_value=0.1, max_value=0.3, step=0.1)))\n","    model.add(Dense(y_train.shape[1]))\n","\n","    model.compile(loss='mse',\n","                  optimizer=Adam(learning_rate=0.001),\n","                  metrics=[MeanSquaredError(), MeanAbsoluteError()])\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"SBi9lHBmOfHF"},"source":["# For Loop"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dmnsl3WhOfHF","executionInfo":{"status":"ok","timestamp":1712931890755,"user_tz":-60,"elapsed":4786491,"user":{"displayName":"Elian Tago","userId":"06354967366807767203"}},"outputId":"41c03c7c-e743-418f-c894-48712b1c2347"},"outputs":[{"output_type":"stream","name":"stdout","text":["Trial 5 Complete [00h 01m 05s]\n","val_loss: 0.011646231636404991\n","\n","Best val_loss So Far: 0.011646231636404991\n","Total elapsed time: 00h 05m 34s\n","\n","    Optimal Hyperparameters\n","        1st Layer Units: 64\n","        2nd Layer Units: 128\n","        No. of Layers: 2\n","        Dropout Rate: 0.1\n","    \n","=========================HOUSE62=========================\n","Train MSE: 0.008943152613937855\n","Train MAE: 0.06005615368485451\n","Test MSE: 0.015453363945637967\n","Test MAE: 0.08870341249527006\n","First Layer: 64\n","Second Layer: 128\n","Number of Layers: 2\n","Dropout Rate: 0.1\n","Time Taken: 348.3961389064789\n","=========================HOUSE62=========================\n","\n"]}],"source":["import time\n","import warnings\n","\n","warnings.filterwarnings('ignore')\n","\n","households = [\n","    2, 3, 6, 9, 11, 12, 16, 17, 20, 21, 22, 23, 25, 28, 29, 30, 31, 33, 34, 36, 39,\n","    41, 42, 46, 49, 50, 53, 57, 61, 62, 64, 73, 77, 81, 84, 85, 90, 91, 94, 98, 100\n","]\n","\n","list_train_MSE = []\n","list_train_MAE = []\n","list_test_MSE = []\n","list_test_MAE = []\n","list_first_layer = []\n","list_second_layer = []\n","list_num_layers = []\n","list_dropoute_rate = []\n","list_time_to_run = []\n","\n","for household in households[15:30]:\n","    start_time = time.time()  # Record the start time of the loop iteration\n","\n","    df = get_household_data(household)\n","    X_train, y_train, X_test, y_test, df_input = data_prep(df)\n","\n","    # Define model checkpoint\n","    cp1 = ModelCheckpoint(f'house{household}.keras', save_best_only=True)\n","\n","    # Initialise Random Search\n","    tuner = RandomSearch(\n","        build_model_hyperparm,\n","        objective='val_loss',\n","        max_trials=5,\n","        executions_per_trial=1,\n","        directory='model_tuning_v2',\n","        project_name=f'house{household}'\n","    )\n","\n","    tuner.search(X_train, y_train, epochs=5, batch_size=16, validation_split=0.1, callbacks=[cp1], verbose=1)\n","\n","    # Get the best hyperparameters\n","    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n","\n","    print(f\"\"\"\n","    Optimal Hyperparameters\n","        1st Layer Units: {best_hps.get('units_1')}\n","        2nd Layer Units: {best_hps.get('units_2')}\n","        No. of Layers: {1 if best_hps.get('return_sequences') == 0 else 2 }\n","        Dropout Rate: {best_hps.get('dropout') }\n","    \"\"\")\n","\n","    # Get the best model\n","    best_model = tuner.get_best_models(num_models=1)[0]\n","    train_mse, train_mae = get_train_metrics(best_model, X_train, y_train)\n","    test_mse, test_mae = get_test_metrics(best_model, X_test, y_test)\n","\n","\n","    list_train_MSE.append(train_mse)\n","    list_train_MAE.append(train_mae)\n","    list_test_MSE.append(test_mse)\n","    list_test_MAE.append(test_mae)\n","    list_first_layer.append(best_hps.get('units_1'))\n","    list_second_layer.append(best_hps.get('units_2'))\n","    list_num_layers.append(1 if best_hps.get('return_sequences') == 0 else 2 )\n","    list_dropoute_rate.append(best_hps.get('dropout'))\n","\n","    end_time = time.time()  # Record the end time of the loop iteration\n","    duration = end_time - start_time  # Calculate the duration\n","\n","    list_time_to_run.append(duration)\n","\n","    print(f\"=========================HOUSE{household}=========================\")\n","    print(f\"Train MSE: {train_mse}\")\n","    print(f\"Train MAE: {train_mae}\")\n","    print(f\"Test MSE: {test_mse}\")\n","    print(f\"Test MAE: {test_mae}\")\n","    print(f\"First Layer: {best_hps.get('units_1')}\")\n","    print(f\"Second Layer: {best_hps.get('units_2')}\")\n","    print(f\"Number of Layers: {1 if best_hps.get('return_sequences') == 0 else 2 }\")\n","    print(f\"Dropout Rate: {best_hps.get('dropout')}\")\n","    print(f\"Time Taken: {duration}\")\n","    print(f\"=========================HOUSE{household}=========================\")\n","    print(\"\")\n","\n","data = {\n","    'Household': households[15:30],\n","    'Train MSE': list_train_MSE,\n","    'Train MAE': list_train_MAE,\n","    'Test MSE': list_test_MSE,\n","    'Test MAE': list_test_MAE,\n","    'First Layer': list_first_layer,\n","    'Second Layer': list_second_layer,\n","    'Num Layers': list_num_layers,\n","    'Dropout Rate': list_dropoute_rate,\n","    'Time to Run': list_time_to_run\n","}\n","\n","\n","df = pd.DataFrame(data)\n","\n","df.to_csv('GRU_results_second_15.csv', index=False)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://github.com/dguiraldes/aml_project/blob/main/LSTM%20For%20Loop%20Final/LSTM_forloop.ipynb","timestamp":1712706854812}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.19"}},"nbformat":4,"nbformat_minor":0}