{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><u> Final LSTM/GRU Layout - Adri and Elian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 144\n",
    "n_features = 5 \n",
    "n_output = 72\n",
    "n_split = 2 * 24 * 324"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Household Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries and Packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "def get_household_data(household):\n",
    "    #Access Credentials\n",
    "    db_host = os.getenv(\"DB_HOST\")\n",
    "    db_name = os.getenv(\"DB_NAME\")\n",
    "    db_user = os.getenv(\"DB_USER\")\n",
    "    db_password = os.getenv(\"DB_PASSWORD\")\n",
    "    db_port = os.getenv(\"DB_PORT\")\n",
    "\n",
    "    conn = psycopg2.connect(\n",
    "    host=db_host,\n",
    "    dbname=db_name,\n",
    "    user=db_user,\n",
    "    password=db_password,\n",
    "    port=db_port\n",
    "    )\n",
    "\n",
    "    query=f\"\"\"\n",
    "    select * \n",
    "    from agg.tidy_data_final\n",
    "    where site = {household}\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    df = df.set_index('timestamp', drop=False)\n",
    "    df = df.sort_index()\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data Set for LSTM/GRU Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "scaler = MaxAbsScaler()\n",
    "\n",
    "def data_prep(df):\n",
    "    global n_input \n",
    "    global n_features\n",
    "    global n_output\n",
    "    global n_split\n",
    "\n",
    "    # Select features of use for later on\n",
    "    df_input = df[['net_load', 'precipitation_probability',\n",
    "                'solar_radiation','sunshine_duration','weekend_or_bank_holiday', \n",
    "                    'month', 'day', 'hour', 'day_of_week', 'season', \n",
    "                    'avg_net_load',]]\n",
    "    \n",
    "    \n",
    "    variables = ['net_load','solar_radiation','sunshine_duration', 'precipitation_probability', 'avg_net_load' ]\n",
    "\n",
    "\n",
    "    for var in variables:\n",
    "        new_var_name = var + '_norm'\n",
    "        df_input[new_var_name] = scaler.fit_transform(df_input[[var]])\n",
    "\n",
    "    df_input['sunshine_duration_norm(t+48)'] = df_input['sunshine_duration_norm'].shift(-48) #Add prediction for time the day after\n",
    "    df_input['solar_radiation_norm(t+48)'] = df_input['solar_radiation_norm'].shift(-48) #Add prediction for time the day after\n",
    "    df_input['precipitation_probability_norm(t+48)'] = df_input['precipitation_probability_norm'].shift(-48) #Add prediction for time the day after\n",
    "\n",
    "    df_input.dropna(inplace=True)\n",
    "\n",
    "    # df_input = df_input[['net_load_norm', 'avg_net_load_norm', 'weekend_or_bank_holiday', 'sunshine_duration_norm(t+48)',\n",
    "    #            'solar_radiation_norm(t+48)','precipitation_probability_norm(t+48)',\n",
    "    #            'month', 'day', 'hour', 'day_of_week', 'season', \n",
    "    #                 ]]\n",
    "    \n",
    "    df_input = df_input[['net_load_norm', 'weekend_or_bank_holiday', 'sunshine_duration_norm(t+48)',\n",
    "            'solar_radiation_norm(t+48)','precipitation_probability_norm(t+48)'\n",
    "                ]]\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(len(df_input) - n_input - n_output + 1):\n",
    "        # Select all columns for the input sequence\n",
    "        X.append(df_input.iloc[i:(i + n_input)].values)  \n",
    "        \n",
    "        y.append(df_input['net_load_norm'].iloc[(i + n_input):(i + n_input + n_output)].values)\n",
    "\n",
    "    X = np.array(X).reshape((len(X), n_input, -1))  \n",
    "    y = np.array(y)\n",
    "\n",
    "    X_train = X[:n_split]\n",
    "    y_train = y[:n_split]\n",
    "\n",
    "    X_test = X[n_split:]\n",
    "    y_test = y[n_split:]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, df_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Fit LSTM/ GRU Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all Necessary Keras Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.losses import MeanAbsoluteError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def build_model(X_train, y_train, model_name):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(GRU(64, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True ))\n",
    "    # model.add(BatchNormalization()) \n",
    "    model.add(LeakyReLU(alpha=0.01)) \n",
    "\n",
    "    model.add(GRU(32))\n",
    "    # model.add(BatchNormalization()) \n",
    "    model.add(LeakyReLU(alpha=0.01)) \n",
    "\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(y_train.shape[1]))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "    #Compile Model\n",
    "    cp1 = ModelCheckpoint(f'{model_name}.keras', save_best_only=True)\n",
    "    model.compile(loss='mse', optimizer=optimizer,  metrics=[MeanSquaredError(), MeanAbsoluteError()])\n",
    "\n",
    "    #Fit Model to Training Data with 10% Validation Split\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=16, validation_split=0.1, callbacks=[cp1])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Training Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_metrics(model, X_train, y_train):\n",
    "    _, train_mse, train_mae = model.evaluate(X_train, y_train, verbose=0)\n",
    "\n",
    "    return train_mse, train_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_viz(start_section, end_section, df_input, X_train, y_train, model):\n",
    "    global n_input \n",
    "    global n_features\n",
    "    global n_output\n",
    "    global n_split\n",
    "\n",
    "\n",
    "\n",
    "    df_ts_train = df_input[n_input:n_split]\n",
    "\n",
    "    X_train_shift = X_train[24:]\n",
    "    y_train_shift = y_train[24:]\n",
    "\n",
    "    X_train_input = X_train_shift[::48]\n",
    "    y_train_input = y_train_shift[::48]\n",
    "\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for i in X_train_input[start_section:end_section]:\n",
    "        i_reshaped = i.reshape(1, n_input, n_features)\n",
    "\n",
    "        prediction = model.predict(i_reshaped)\n",
    "\n",
    "        # We want to omit the first 12 hours the predictions i.e., the non-shaded squares in the figure above.\n",
    "        prediction_list = list(prediction[0][24:])\n",
    "        predictions.append(prediction_list)\n",
    "\n",
    "\n",
    "    y_train_input_24h = []\n",
    "\n",
    "    for i in y_train_input[start_section:end_section]:\n",
    "        # i_reshaped = i.reshape(1, n_input, n_features)\n",
    "        y_train_input_24h.append(i[24:])\n",
    "        \n",
    "\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.title(f\"Date Range: {df_ts_train.index[start_section*48+48].strftime('%d/%m/%Y')} - {df_ts_train.index[end_section*48+48].strftime('%d/%m/%Y')}\")\n",
    "\n",
    "    # We want to plot the predictions from 00:00 to 00:00 the next day.\n",
    "    plt.plot(df_ts_train.index[start_section*48+48: end_section*48+48], np.array(predictions).flatten(), label='Prediction')\n",
    "    plt.plot(df_ts_train.index[start_section*48+48: end_section*48+48], np.array(y_train_input_24h).flatten(), label = 'Actual')\n",
    "    plt.xlabel('Datetime')\n",
    "    plt.ylabel('Normalised Net Load (-)')\n",
    "\n",
    "\n",
    "    # Generate vertical lines for when we predict\n",
    "    xcoords_DAM = df_ts_train.index[start_section*48+72: (end_section)*48+72][::48]\n",
    "\n",
    "    for i, xc in enumerate(xcoords_DAM):\n",
    "        if i == 0:  # First item gets the label\n",
    "            plt.axvline(x=xc, color='red', linestyle='--', alpha=0.5, label='DAM Gate Closure')\n",
    "        else:  # Subsequent items do not\n",
    "            plt.axvline(x=xc, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "\n",
    "\n",
    "    # Generate vertical lines for DAM closure\n",
    "    xcoords_pred = df_ts_train.index[start_section*48+48: (end_section+1)*48+48][::48]\n",
    "\n",
    "    for i, xc in enumerate(xcoords_pred):\n",
    "        if i == 0:  # First item gets the label\n",
    "            plt.axvline(x=xc, color='purple', linestyle='--', alpha=0.3, label='Energy Delivery Prediction')\n",
    "        else:  # Subsequent items do not\n",
    "            plt.axvline(x=xc, color='purple', linestyle='--', alpha=0.3)\n",
    "\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(0.5, 1.15), loc='center', borderaxespad=0., ncol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Test Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def get_test_metrics(model, X_test, y_test):\n",
    "    global n_input \n",
    "    global n_features\n",
    "    global n_output\n",
    "    global n_split\n",
    "\n",
    "    X_test_shift = X_test[24:]\n",
    "    y_test_shift = y_test[24:]\n",
    "\n",
    "    X_test_input = X_test_shift[::48]\n",
    "    y_test_input = y_test_shift[::48]\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for i in X_test_input:\n",
    "        i_reshaped = i.reshape(1, n_input, n_features)\n",
    "\n",
    "        prediction = model.predict(i_reshaped, verbose=0)\n",
    "\n",
    "        # We want to omit the first 12 hours the predictions i.e., the non-shaded squares in the figure above.\n",
    "        prediction_list = list(prediction[0][24:])\n",
    "        predictions.append(prediction_list)\n",
    "\n",
    "\n",
    "    y_test_input_24h = []\n",
    "\n",
    "    for i in y_test_input:\n",
    "        # i_reshaped = i.reshape(1, n_input, n_features)\n",
    "        y_test_input_24h.append(i[24:])\n",
    "\n",
    "    # Flatten the lists of lists into single lists\n",
    "    predictions_flat = [item for sublist in predictions for item in sublist]\n",
    "    y_test_flat = [item for sublist in y_test_input_24h for item in sublist]\n",
    "\n",
    "    # Calculate MSE\n",
    "    test_mse = mean_squared_error(y_test_flat, predictions_flat)\n",
    "\n",
    "    # Calculate MAE\n",
    "    test_mae = mean_absolute_error(y_test_flat, predictions_flat)\n",
    "\n",
    "    return test_mse, test_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testing_viz(start_section, end_section, df_input, X_test, y_test, model):\n",
    "    global n_input \n",
    "    global n_features\n",
    "    global n_output\n",
    "    global n_split\n",
    "\n",
    "    df_ts_test = df_input[n_input+n_split:]\n",
    "\n",
    "    X_test_shift = X_test[24:]\n",
    "    y_test_shift = y_test[24:]\n",
    "\n",
    "    X_test_input = X_test_shift[::48]\n",
    "    y_test_input = y_test_shift[::48]\n",
    "\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for i in X_test_input[start_section:end_section]:\n",
    "        i_reshaped = i.reshape(1, n_input, n_features)\n",
    "\n",
    "        prediction = model.predict(i_reshaped)\n",
    "\n",
    "        # We want to omit the first 12 hours the predictions i.e., the non-shaded squares in the figure above.\n",
    "        prediction_list = list(prediction[0][24:])\n",
    "        predictions.append(prediction_list)\n",
    "\n",
    "\n",
    "    y_test_input_24h = []\n",
    "\n",
    "    for i in y_test_input[start_section:end_section]:\n",
    "        # i_reshaped = i.reshape(1, n_input, n_features)\n",
    "        y_test_input_24h.append(i[24:])\n",
    "        \n",
    "\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.title(f\"Date Range: {df_ts_test.index[start_section*48+48].strftime('%d/%m/%Y')} - {df_ts_test.index[end_section*48+48].strftime('%d/%m/%Y')}\")\n",
    "\n",
    "    # We want to plot the predictions from 00:00 to 00:00 the next day.\n",
    "    plt.plot(df_ts_test.index[start_section*48+48: end_section*48+48], np.array(predictions).flatten(), label='Prediction')\n",
    "    plt.plot(df_ts_test.index[start_section*48+48: end_section*48+48], np.array(y_test_input_24h).flatten(), label = 'Actual')\n",
    "    plt.xlabel('Datetime')\n",
    "    plt.ylabel('Normalised Net Load (-)')\n",
    "\n",
    "\n",
    "    # Generate vertical lines for when we predict\n",
    "    xcoords_DAM = df_ts_test.index[start_section*48+72: (end_section)*48+72][::48]\n",
    "\n",
    "    for i, xc in enumerate(xcoords_DAM):\n",
    "        if i == 0:  # First item gets the label\n",
    "            plt.axvline(x=xc, color='red', linestyle='--', alpha=0.5, label='DAM Gate Closure')\n",
    "        else:  # Subsequent items do not\n",
    "            plt.axvline(x=xc, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "\n",
    "\n",
    "    # Generate vertical lines for DAM closure\n",
    "    xcoords_pred = df_ts_test.index[start_section*48+48: (end_section+1)*48+48][::48]\n",
    "\n",
    "    for i, xc in enumerate(xcoords_pred):\n",
    "        if i == 0:  # First item gets the label\n",
    "            plt.axvline(x=xc, color='purple', linestyle='--', alpha=0.3, label='Energy Delivery Prediction')\n",
    "        else:  # Subsequent items do not\n",
    "            plt.axvline(x=xc, color='purple', linestyle='--', alpha=0.3)\n",
    "\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(0.5, 1.15), loc='center', borderaxespad=0., ncol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n_/s_vk866s6ds93jqhjv__r6cr0000gn/T/ipykernel_59967/894429879.py:6: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import RandomSearch\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import MeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "def build_model_hyperparm(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(GRU(hp.Int('units_1', min_value=32, max_value=256, step=32),\n",
    "                   input_shape=(X_train.shape[1], X_train.shape[2]), \n",
    "                   return_sequences=hp.Choice('return_sequences', [True, False])))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "    if hp.Choice('return_sequences', [True, False]):\n",
    "        model.add(GRU(hp.Int('units_2', min_value=32, max_value=256, step=32)))\n",
    "        model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "    model.add(Dropout(hp.Float('dropout', min_value=0.1, max_value=0.3, step=0.1)))\n",
    "    model.add(Dense(y_train.shape[1]))\n",
    "    \n",
    "    model.compile(loss='mse', \n",
    "                  optimizer=Adam(learning_rate=0.001), \n",
    "                  metrics=[MeanSquaredError(), MeanAbsoluteError()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 03m 03s]\n",
      "val_loss: 0.019516684114933014\n",
      "\n",
      "Best val_loss So Far: 0.018274111673235893\n",
      "Total elapsed time: 02h 56m 20s\n",
      "\n",
      "    Optimal Hyperparameters\n",
      "        1st Layer Units: 192\n",
      "        2nd Layer Units: 96\n",
      "        No. of Layers: 2\n",
      "        Dropout Rate: 0.2\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================HOUSE11=========================\n",
      "Train MSE: 0.012620370835065842\n",
      "Train MAE: 0.07439977675676346\n",
      "Test MSE: 0.020285886226722685\n",
      "Test MAE: 0.09987080285857959\n",
      "First Layer: 192\n",
      "Second Layer: 96\n",
      "Number of Layers: 2\n",
      "Dropout Rate: 0.2\n",
      "Time Taken: 10604.61445403099\n",
      "=========================HOUSE11=========================\n",
      "\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 100\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHousehold\u001b[39m\u001b[38;5;124m'\u001b[39m: households,\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain MSE\u001b[39m\u001b[38;5;124m'\u001b[39m: list_train_MSE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime to Run\u001b[39m\u001b[38;5;124m'\u001b[39m: list_time_to_run\n\u001b[1;32m     97\u001b[0m }\n\u001b[0;32m--> 100\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n\u001b[1;32m    102\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGRU_results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/pandas/core/frame.py:733\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    727\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    728\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    729\u001b[0m     )\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, typ\u001b[38;5;241m=\u001b[39mmanager)\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[38;5;241m=\u001b[39mdtype, typ\u001b[38;5;241m=\u001b[39mtyp, consolidate\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/pandas/core/internals/construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "households = [\n",
    "    2, 3, 6, 9, 11, 12, 16, 17, 20, 21, 22, 23, 25, 28, 29, 30, 31, 33, 34, 36, 39,\n",
    "    41, 42, 46, 49, 50, 53, 57, 61, 62, 64, 73, 77, 81, 84, 85, 90, 91, 94, 98, 100\n",
    "]\n",
    "\n",
    "list_train_MSE = []\n",
    "list_train_MAE = []\n",
    "list_test_MSE = []\n",
    "list_test_MAE = []\n",
    "list_first_layer = []\n",
    "list_second_layer = []\n",
    "list_num_layers = []\n",
    "list_dropoute_rate = []\n",
    "list_time_to_run = []\n",
    "\n",
    "for household in households[0:15]:\n",
    "    start_time = time.time()  # Record the start time of the loop iteration\n",
    "\n",
    "    df = get_household_data(household)\n",
    "    X_train, y_train, X_test, y_test, df_input = data_prep(df)\n",
    "\n",
    "    # Define model checkpoint\n",
    "    cp1 = ModelCheckpoint(f'house{household}.keras', save_best_only=True)\n",
    "\n",
    "    # Initialise Random Search\n",
    "    tuner = RandomSearch(\n",
    "        build_model_hyperparm,\n",
    "        objective='val_loss',\n",
    "        max_trials=5,  \n",
    "        executions_per_trial=1,  \n",
    "        directory='model_tuning_v3',\n",
    "        project_name=f'house{household}'\n",
    "    )\n",
    "\n",
    "    tuner.search(X_train, y_train, epochs=3, batch_size=16, validation_split=0.1, callbacks=[cp1], verbose=1)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    print(f\"\"\"\n",
    "    Optimal Hyperparameters\n",
    "        1st Layer Units: {best_hps.get('units_1')}\n",
    "        2nd Layer Units: {best_hps.get('units_2')}\n",
    "        No. of Layers: {1 if best_hps.get('return_sequences') == 0 else 2 }\n",
    "        Dropout Rate: {best_hps.get('dropout') }\n",
    "    \"\"\")\n",
    "\n",
    "    # Get the best model\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "    train_mse, train_mae = get_train_metrics(best_model, X_train, y_train)\n",
    "    test_mse, test_mae = get_test_metrics(best_model, X_test, y_test)\n",
    "\n",
    "\n",
    "    list_train_MSE.append(train_mse)\n",
    "    list_train_MAE.append(train_mae)\n",
    "    list_test_MSE.append(test_mse)\n",
    "    list_test_MAE.append(test_mae)\n",
    "    list_first_layer.append(best_hps.get('units_1'))\n",
    "    list_second_layer.append(best_hps.get('units_2'))\n",
    "    list_num_layers.append(1 if best_hps.get('return_sequences') == 0 else 2 )\n",
    "    list_dropoute_rate.append(best_hps.get('dropout'))\n",
    "\n",
    "    end_time = time.time()  # Record the end time of the loop iteration\n",
    "    duration = end_time - start_time  # Calculate the duration\n",
    "\n",
    "    list_time_to_run.append(duration)\n",
    "\n",
    "    print(f\"=========================HOUSE{household}=========================\")\n",
    "    print(f\"Train MSE: {train_mse}\")\n",
    "    print(f\"Train MAE: {train_mae}\")\n",
    "    print(f\"Test MSE: {test_mse}\")\n",
    "    print(f\"Test MAE: {test_mae}\")\n",
    "    print(f\"First Layer: {best_hps.get('units_1')}\")\n",
    "    print(f\"Second Layer: {best_hps.get('units_2')}\")\n",
    "    print(f\"Number of Layers: {1 if best_hps.get('return_sequences') == 0 else 2 }\")\n",
    "    print(f\"Dropout Rate: {best_hps.get('dropout')}\")\n",
    "    print(f\"Time Taken: {duration}\")\n",
    "    print(f\"=========================HOUSE{household}=========================\")\n",
    "    print(\"\")\n",
    "\n",
    "data = {\n",
    "    'Household': households[0:15],\n",
    "    'Train MSE': list_train_MSE,\n",
    "    'Train MAE': list_train_MAE,\n",
    "    'Test MSE': list_test_MSE,\n",
    "    'Test MAE': list_test_MAE,\n",
    "    'First Layer': list_first_layer,\n",
    "    'Second Layer': list_second_layer,\n",
    "    'Num Layers': list_num_layers,\n",
    "    'Dropout Rate': list_dropoute_rate,\n",
    "    'Time to Run': list_time_to_run\n",
    "}\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.to_csv('GRU_results.csv', index=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
