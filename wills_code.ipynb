{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example to connect to postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a tutorial to connect to our PostgreSQL database using python.\n",
    "\n",
    "First you need to install the following libraries:\n",
    "- psycopg2\n",
    "- python-dotenv\n",
    "\n",
    "Then you need to create a file with the name \".env\". This file will contain the connection information and your credentials. This is an example:\n",
    "\n",
    "```\n",
    "DB_HOST=host_name\n",
    "DB_NAME=postgres\n",
    "DB_USER=my_user\n",
    "DB_PASSWORD=my_password\n",
    "DB_PORT=5432\n",
    "```\n",
    "\n",
    "After that youÂ´re all set. We will import your credentials and connect to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2 # PostgreSQL database adapter for Python\n",
    "from dotenv import load_dotenv # Reads the key-value pair from .env file and adds them to environment variable\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Accessing credentials\n",
    "db_host = os.getenv(\"DB_HOST\")\n",
    "db_name = os.getenv(\"DB_NAME\")\n",
    "db_user = os.getenv(\"DB_USER\")\n",
    "db_password = os.getenv(\"DB_PASSWORD\")\n",
    "db_port = os.getenv(\"DB_PORT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "conn = psycopg2.connect(\n",
    "    host=db_host,\n",
    "    dbname=db_name,\n",
    "    user=db_user,\n",
    "    password=db_password,\n",
    "    port=db_port\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can query our data (write sql code) and store it as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\"\n",
    "    select * \n",
    "    from agg.tidy_data_final  \n",
    "    where year between 2019 and 2020\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average hourly\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "df = df.drop('timestamp', axis=1)\n",
    "#df = df.groupby(['year','month', 'day', 'hour','minute']).mean().reset_index()\n",
    "#df['net_load_norm'] = df['net_load'] / max(abs(df['net_load']))\n",
    "df['datetime'] = pd.to_datetime(df[['year','month', 'day', 'hour','minute']])\n",
    "df = df.sort_values(by='datetime')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['net_load', 'datetime','solar_radiation','sunshine_duration','precipitation_probability','site','weekend_or_bank_holiday','hour']\n",
    "\n",
    "# Keep only the specified columns\n",
    "df = df.loc[:, columns_to_keep]\n",
    "df['net_load'] = df['net_load'].astype('float32')\n",
    "df['solar_radiation'] = df['solar_radiation'].astype('float32')\n",
    "df['sunshine_duration'] = df['sunshine_duration'].astype('float32')\n",
    "df['precipitation_probability'] = df['precipitation_probability'].astype('float32')\n",
    "\n",
    "def normalize_net_load(group):\n",
    "    max_net_load = abs(group['net_load']).max()  # Get maximum net load value for the site\n",
    "    group['net_load_norm'] = group['net_load'] / max_net_load  # Calculate normalized net load\n",
    "    return group\n",
    "\n",
    "# Apply normalization function to each site group using transform()\n",
    "df['net_load_n2'] = df.groupby('site')['net_load'].transform(lambda x: x / abs(x).max())\n",
    "\n",
    "\n",
    "\n",
    "print(df)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_20_data = df[df['site'] == 6]\n",
    "plt.plot(site_20_data['datetime'], site_20_data['net_load_n2'], label='Net Load (Site 20)', color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['datetime'] >= '2019-11-25'] # df = df[df['datetime'] >= '2020-1-25']\n",
    "df = df[df['datetime'] <= '2020-3-26']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_load_day = df.groupby(df['datetime'].dt.date)['net_load_n2'].mean()\n",
    "plt.plot(avg_load_day)\n",
    "plt.xticks(rotation=45) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "df = df.sort_values(by='datetime')\n",
    "\n",
    "# Plot the time series data\n",
    "plt.plot(df['datetime'],df['net_load'])\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('net_load_norm')\n",
    "plt.title('Time Series Data')\n",
    "plt.show()\n",
    "\n",
    "# Plot autocorrelation and partial autocorrelation plots\n",
    "'''\n",
    "plot_acf(df['net_load'])\n",
    "plt.title('Autocorrelation Plot')\n",
    "plt.show()\n",
    "\n",
    "plot_pacf(df['net_load'])\n",
    "plt.title('Partial Autocorrelation Plot')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Run the arim model and get the equation should speed everthing up and also be more complex\n",
    "'''\n",
    "df = df.sort_values(by='datetime')\n",
    "results = pd.DataFrame()\n",
    "for x in range(4,0,-1):\n",
    "    vals = x*-24\n",
    "    train_data = df.iloc[:vals]  \n",
    "    test_data = df.iloc[vals:]\n",
    "    test_data = test_data.iloc[:24]\n",
    "\n",
    "    model = SARIMAX(train_data['net_load_norm'], exog=train_data[['solar_radiation','weekend_or_bank_holiday']], order=(4, 1, 3), seasonal_order=(2, 1, 1, 24))\n",
    "    sarimax_model = model.fit()\n",
    "\n",
    "    sarimax_params = sarimax_model.params\n",
    "\n",
    "    print(\"SARIMAX Parameters:\")\n",
    "    print(sarimax_params)\n",
    "\n",
    "    forecast_horizon = 24\n",
    "    forecast = sarimax_model.forecast(steps=forecast_horizon, exog=test_data[['solar_radiation','weekend_or_bank_holiday']])\n",
    "    #print('Forecasted Values:', forecast)\n",
    "    df_final = pd.DataFrame()\n",
    "    df_final = pd.concat([test_data, forecast], axis=1)\n",
    "    print(df_final)\n",
    "\n",
    "    results = pd.concat([results, df_final],ignore_index=True)\n",
    "\n",
    "results = results.rename(columns={results.columns[-1]: 'forecast'})\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['datetime'], df['net_load_norm'], label='Observed')\n",
    "plt.plot(results['datetime'], results['forecast'], label='Forecast', color='red')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('SARIMAX Model Forecast')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45) \n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df2 = df[-300:]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df2['datetime'], df2['net_load'], label='Observed')\n",
    "plt.plot(results['datetime'], results['forecast'], label='Forecast', color='red')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('SARIMAX Model Forecast')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45) \n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pmdarima as pm\n",
    "from tqdm import tqdm\n",
    "'''\n",
    "# Run the auto arima get the params then run the non auto with the params for each it\n",
    "\n",
    "# Assuming 'df' is your dataframe containing the data\n",
    "train_data = df.iloc[:-336]\n",
    "\n",
    "# Define ARIMA parameters ( need to trial q = 5)\n",
    "start_p = 1  \n",
    "start_d = 1  \n",
    "start_q = 1  \n",
    "max_p = 3  \n",
    "max_d = 2  \n",
    "max_q = 3  \n",
    "\n",
    "start_P = 1  \n",
    "start_D = 1  \n",
    "start_Q = 2  \n",
    "max_P = 1  \n",
    "max_D = 1  \n",
    "max_Q = 3  \n",
    "\n",
    "\n",
    "# Initialize tqdm progress bar\n",
    "with tqdm(total=1) as pbar:\n",
    "    def progress_callback(iteration):\n",
    "        pbar.update(1)\n",
    "\n",
    "    # Train ARIMA model with progress callback\n",
    "    model_auto = pm.auto_arima(\n",
    "        train_data['net_load_norm'],\n",
    "        exogenous=train_data[['solar_radiation','sunshine_duration', 'precipitation_probability']],\n",
    "        seasonal=True,\n",
    "        m=48,\n",
    "        start_p=start_p,\n",
    "        start_d=start_d,\n",
    "        start_q=start_q,\n",
    "        max_d=max_d,\n",
    "        max_q=max_q,\n",
    "        start_P=start_P,\n",
    "        start_D=start_D,\n",
    "        start_Q=start_Q,\n",
    "        max_D=max_D,\n",
    "        max_Q=max_Q,\n",
    "        stepwise=True,\n",
    "        method='nm',\n",
    "        maxiter=50,\n",
    "        trace=True,\n",
    "        callback=progress_callback\n",
    "    )\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_daytime(hour):\n",
    "    if hour <= 6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def assign_daytime2(hour):\n",
    "    if 6 < hour <= 10:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def assign_daytime3(hour):\n",
    "    if 10 < hour <= 16:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def assign_daytime4(hour):\n",
    "    if 16 < hour <= 21:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def assign_daytime5(hour):\n",
    "    if 21 < hour:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['time1'] = df['hour'].apply(assign_daytime)\n",
    "df['time2'] = df['hour'].apply(assign_daytime2)\n",
    "df['time3'] = df['hour'].apply(assign_daytime3)\n",
    "df['time4'] = df['hour'].apply(assign_daytime4)\n",
    "df['time5'] = df['hour'].apply(assign_daytime5)\n",
    "\n",
    "'''\n",
    "def assign_daytime(hour):\n",
    "    if hour <= 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def assign_daytime2(hour):\n",
    "    if 2 < hour <= 4:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def assign_daytime3(hour):\n",
    "    if 4 < hour <= 6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def assign_daytime4(hour):\n",
    "    if 6 < hour <= 8:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def assign_daytime5(hour):\n",
    "    if 8 < hour <= 10:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def assign_daytime6(hour):\n",
    "    if 10 < hour <= 12:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def assign_daytime7(hour):\n",
    "    if 12 < hour <= 14:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def assign_daytime8(hour):\n",
    "    if 14 < hour <= 16:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def assign_daytime9(hour):\n",
    "    if 16 < hour <= 18:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def assign_daytime10(hour):\n",
    "    if 18 < hour <= 20:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def assign_daytime11(hour):\n",
    "    if 20 < hour <= 22:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def assign_daytime12(hour):\n",
    "    if 22 < hour:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['time1'] = df['hour'].apply(assign_daytime)\n",
    "df['time2'] = df['hour'].apply(assign_daytime2)\n",
    "df['time3'] = df['hour'].apply(assign_daytime3)\n",
    "df['time4'] = df['hour'].apply(assign_daytime4)\n",
    "df['time5'] = df['hour'].apply(assign_daytime5)\n",
    "df['time6'] = df['hour'].apply(assign_daytime6)\n",
    "df['time7'] = df['hour'].apply(assign_daytime7)\n",
    "df['time8'] = df['hour'].apply(assign_daytime8)\n",
    "df['time9'] = df['hour'].apply(assign_daytime9)\n",
    "df['time10'] = df['hour'].apply(assign_daytime10)\n",
    "df['time11'] = df['hour'].apply(assign_daytime11)\n",
    "df['time12'] = df['hour'].apply(assign_daytime12)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ARIMAorder(data):\n",
    "\n",
    "    fit = pm.auto_arima(\n",
    "        data['net_load_norm'],\n",
    "        exogenous=data[['solar_radiation','sunshine_duration', 'precipitation_probability','weekend_or_bank_holiday']],\n",
    "        seasonal=True,\n",
    "        m=48,\n",
    "        stepwise=True,\n",
    "        method='nm',\n",
    "        maxiter=50,\n",
    "        trace=True,\n",
    "        start_p=1,\n",
    "        start_q=1,\n",
    "        max_p=2,\n",
    "        max_q=2,\n",
    "        max_d=2,\n",
    "        start_P=1,\n",
    "        start_Q=1,\n",
    "        max_P=2,\n",
    "        max_Q=2,\n",
    "        max_D=2,\n",
    "    )\n",
    "    return fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "'month','day','hour','isweekend','solar radiation', 'sunshine duration','precipitation probability'\n",
    "'''    start_p=1,\n",
    "        start_d=1,\n",
    "        start_q=1,\n",
    "        max_p=3,\n",
    "        max_d=2,\n",
    "        max_q=3,\n",
    "        start_P=1,\n",
    "        start_D=1,\n",
    "        start_Q=1,\n",
    "        max_P=2,\n",
    "        max_D=2,\n",
    "        max_Q=2,\n",
    "        '''\n",
    "def ARIMAorder(data):\n",
    "\n",
    "    fit = pm.auto_arima(\n",
    "        data['net_load_n2'],\n",
    "        exogenous=data[['solar_radiation','sunshine_duration', 'precipitation_probability','weekend_or_bank_holiday','time1','time2','time3','time4','time5']],\n",
    "        stepwise=True,\n",
    "        method='nm',\n",
    "        maxiter=100,\n",
    "        trace=True,\n",
    "        start_p=1,\n",
    "        start_q=1,\n",
    "        start_d=1,\n",
    "        max_p=5,\n",
    "        max_q=3,\n",
    "        max_d=5\n",
    "    )\n",
    "    return fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE BIG LOOP\n",
    "em = ['site','MAE','MSE']\n",
    "em_df = pd.DataFrame(columns=em)\n",
    "houses = [2, 3,6, 9, 11, 12, 16] \n",
    "#houses = [17, 20, 21, 22]\n",
    "#houses = [23, 25] [28!!!!!] order = (1,0,1), seasonal_order=(2,0,1,48)) [29]\n",
    "#houses = [30, 31, 33] #34, 36, 39,41, 42, 46, 49, 50, 53, 57, 61, 62, 64, 73, 77, 81, 84, 85, 90, 91, 94, 98, 100]\n",
    "\n",
    "# loop for each site\n",
    "for i in houses:\n",
    "    site_df = df[df['site']==i]\n",
    "    #site_df['net_load_norm'] = site_df['net_load'] / max(abs(site_df['net_load']))\n",
    "    site_df_train = site_df[:-1464]\n",
    "    print('House:',i)\n",
    "    # run the auto arima and extract the optimal params \n",
    "    fit = ARIMAorder(site_df_train)\n",
    "    results = pd.DataFrame()\n",
    "    \n",
    "    # Loop for each day\n",
    "    for x in range(31,1,-1):\n",
    "        # get the most up to date data for this predicton\n",
    "        vals = -x*48 +24 #to get midday\n",
    "        train_data = site_df.iloc[:vals]  \n",
    "        test_data = site_df.iloc[vals:]\n",
    "        test_data = test_data.iloc[:72]\n",
    "\n",
    "        # Create the model with the new data\n",
    "        model_auto = SARIMAX(train_data['net_load_n2'], exog=train_data[['solar_radiation','sunshine_duration', 'precipitation_probability','weekend_or_bank_holiday','time1','time2','time3','time4','time5']], order= fit.get_params().get(\"order\"))#,seasonal_order=fit.get_params().get(\"seasonal_order\"))\n",
    "        sarimax_model = model_auto.fit()\n",
    "\n",
    "        # Forecast the next period\n",
    "        forecast_horizon = 72\n",
    "        forecast = sarimax_model.forecast(steps=forecast_horizon, exog=test_data[['solar_radiation','sunshine_duration', 'precipitation_probability','weekend_or_bank_holiday','time1','time2','time3','time4','time5']])\n",
    "        df_final = pd.DataFrame(test_data)\n",
    "        forecast_df = pd.DataFrame(forecast)\n",
    "        df_final['forecast'] = forecast_df['predicted_mean'].values\n",
    "        df_final = df_final[24:]\n",
    "        results = pd.concat([results, df_final],ignore_index=True)\n",
    "        print(x-1)\n",
    "\n",
    "    # add the evaluation metric for the site to a df\n",
    "    norm_mae = round(mean_absolute_error(results['net_load_n2'], results['forecast']),4)\n",
    "    norm_mse = round(mean_squared_error(results['net_load_n2'], results['forecast']),4)\n",
    "    house_res = {'site': i, 'MAE': norm_mae, 'MSE': norm_mse}\n",
    "    house_res_df = pd.DataFrame([house_res])\n",
    "    em_df= pd.concat([em_df, house_res_df],ignore_index=True)\n",
    "\n",
    "\n",
    "    # Plot the figure for each house \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(site_df['datetime'], site_df['net_load_n2'], label='Observed')\n",
    "    plt.plot(results['datetime'], results['forecast'], label='Forecast', color='red')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('SARIMAX Model Forecast')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45) \n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(em_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/xlow6/MSc_ESDA/2.AML/em.csv'\n",
    "# Save DataFrame to CSV\n",
    "em_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = site_df[site_df['datetime'] <= '2020-3-26']\n",
    "df2 = df2[df2['datetime'] >= '2020-3-1']\n",
    "\n",
    "results2 = results[results['datetime'] <= '2020-3-26']\n",
    "results2 = results2[results2['datetime'] >= '2020-3-1']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df2['datetime'], df2['net_load_norm'], label='Observed',alpha=0.6)\n",
    "plt.plot(results2['datetime'], results2['forecast'], label='Forecast', color='red',alpha=1)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('SARIMAX Model Forecast')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45) \n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "date_range1 = pd.date_range(start='2020-03-01', end='2020-03-06', freq='D')\n",
    "\n",
    "\n",
    "# Plot each date with a vertical line at 12:00\n",
    "for date in date_range1:\n",
    "    plt.axvline(x=date + pd.Timedelta(hours=12), color='red', linestyle='--', alpha=0.5)\n",
    "    plt.axvline(x=date, color='black', linestyle='--', alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(site_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = round(mean_squared_error(results['net_load_norm'], results['forecast']),4)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 36 Hour forecast\n",
    "## Train model and save then load model and update each time \n",
    "'''\n",
    "results = pd.DataFrame()\n",
    "for x in range(30,1,-1):\n",
    "    vals = -x*48 +24 #to get midday\n",
    "    train_data = df.iloc[:vals]  \n",
    "    test_data = df.iloc[vals:]\n",
    "    test_data = test_data.iloc[:72]\n",
    "\n",
    "    #model_auto = model_auto.update(train_data['net_load_norm'], exog=train_data[['solar_radiation','sunshine_duration', 'precipitation_probability']], maxiter=1,trace=True,inplace=True)\n",
    "    #model_auto.update(train_data['net_load_norm'])\n",
    "\n",
    "    model_auto = SARIMAX(train_data['net_load_norm'], exog=train_data[['solar_radiation','sunshine_duration', 'precipitation_probability']], order=(1, 1, 1), seasonal_order=(1, 0, 0, 48))\n",
    "    sarimax_model = model_auto.fit()\n",
    "\n",
    "    #sarimax_params = model_auto.params\n",
    "\n",
    "    #print(\"SARIMAX Parameters:\")\n",
    "    #print(sarimax_params)\n",
    "\n",
    "    forecast_horizon = 72\n",
    "    forecast = sarimax_model.forecast(steps=forecast_horizon, exog=test_data[['solar_radiation','sunshine_duration', 'precipitation_probability']])\n",
    "    #forecast, conf_int = model_auto.predict(n_periods=72, exogenous=test_data[['solar_radiation','sunshine_duration', 'precipitation_probability']], return_conf_int=True, order=(1, 1, 1), seasonal_order=(1, 0, 0, 48))\n",
    "    #forecast = model_auto.predict(n_periods=72, exogenous=test_data[['solar_radiation','sunshine_duration', 'precipitation_probability']], return_conf_int=True, order=(1, 1, 1), seasonal_order=(1, 0, 0, 48))\n",
    "    df_final = pd.DataFrame(test_data)\n",
    "    forecast_df = pd.DataFrame(forecast)\n",
    "    #print(df_final)\n",
    "    #print(forecast_df)\n",
    "    df_final['forecast'] = forecast_df['predicted_mean'].values\n",
    "    df_final = df_final[24:]\n",
    "    #print(conf_int)\n",
    "\n",
    "    results = pd.concat([results, df_final],ignore_index=True)\n",
    "    print(x-1)\n",
    "\n",
    "#results = results.rename(columns={results.columns[-1]: 'forecast'})\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['datetime'], df['net_load_norm'], label='Observed')\n",
    "plt.plot(results['datetime'], results['forecast'], label='Forecast', color='red')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('SARIMAX Model Forecast')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45) \n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "#INdexing and setting params in prediction line\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "## Day Forecast\n",
    "results = pd.DataFrame()\n",
    "for x in range(4,0,-1):\n",
    "    vals = -x*48\n",
    "    train_data = df.iloc[:vals]  \n",
    "    test_data = df.iloc[vals:]\n",
    "    test_data = test_data.iloc[:48]\n",
    "\n",
    "    model_auto.update(train_data['net_load_norm'], exog=train_data[['solar_radiation','weekend_or_bank_holiday','sunshine_duration', 'precipitation_probability']], maxiter=1)\n",
    "    #model_auto.update(train_data['net_load_norm'])\n",
    "\n",
    "    sarimax_params = model_auto.params\n",
    "\n",
    "    print(\"SARIMAX Parameters:\")\n",
    "    print(sarimax_params)\n",
    "\n",
    "    #forecast_horizon = 24\n",
    "    #forecast = model.forecast(steps=forecast_horizon, exog=test_data[['solar_radiation','weekend_or_bank_holiday']])\n",
    "    forecast, conf_int = model_auto.predict(n_periods=48, exogenous=test_data[['solar_radiation','weekend_or_bank_holiday','sunshine_duration', 'precipitation_probability']], return_conf_int=True)\n",
    "    df_final = pd.DataFrame(test_data)\n",
    "    df_final['forecast'] = forecast \n",
    "\n",
    "    results = pd.concat([results, df_final],ignore_index=True)\n",
    "    print(x)\n",
    "\n",
    "results = results.rename(columns={results.columns[-1]: 'forecast'})\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['datetime'], df['net_load_norm'], label='Observed')\n",
    "plt.plot(results['datetime'], results['forecast'], label='Forecast', color='red')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('SARIMAX Model Forecast')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45) \n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[-1440:]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df2['datetime'], df2['net_load_norm'], label='Observed',alpha=0.6)\n",
    "plt.plot(results['datetime'], results['forecast'], label='Forecast', color='red',alpha=1)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('SARIMAX Model Forecast')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45) \n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "#plt.axvline(x=pd.to_datetime('2020-3-24 12:00:00'), color='black', linestyle='--',linewidth=1)\n",
    "\n",
    "start_date = pd.Timestamp('2020-02-26')  # Replace with your desired start date\n",
    "end_date = pd.Timestamp('2020-03-26')    # Replace with your desired end date\n",
    "plt.xlim(start_date, end_date)\n",
    "plt.ylim(-1,1)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "norm_mae = mean_absolute_error(results['net_load_norm'], results['forecast'])\n",
    "print(round(norm_mae,4))\n",
    "\n",
    "def mean_absolute_arctangent_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs(np.arctan((y_true - y_pred) / np.abs(y_true)) / (np.pi / 2)))\n",
    "\n",
    "maape = mean_absolute_arctangent_percentage_error(results['net_load_norm'], results['forecast'])\n",
    "print(round(maape,3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
