{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><u> Final LSTM/GRU Layout - Adri and Elian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "household = 20\n",
    "model_name = 'house20_function_tests'\n",
    "n_input = 144\n",
    "n_features = 11 \n",
    "n_output = 72\n",
    "n_split = 2 * 24 * 324\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Household Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries and Packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "def get_household_data(household):\n",
    "    #Access Credentials\n",
    "    db_host = os.getenv(\"DB_HOST\")\n",
    "    db_name = os.getenv(\"DB_NAME\")\n",
    "    db_user = os.getenv(\"DB_USER\")\n",
    "    db_password = os.getenv(\"DB_PASSWORD\")\n",
    "    db_port = os.getenv(\"DB_PORT\")\n",
    "\n",
    "    conn = psycopg2.connect(\n",
    "    host=db_host,\n",
    "    dbname=db_name,\n",
    "    user=db_user,\n",
    "    password=db_password,\n",
    "    port=db_port\n",
    "    )\n",
    "\n",
    "    query=f\"\"\"\n",
    "    select * \n",
    "    from agg.tidy_data_final\n",
    "    where site = {household}\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    df = df.set_index('timestamp', drop=False)\n",
    "    df = df.sort_index()\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data Set for LSTM/GRU Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "scaler = MaxAbsScaler()\n",
    "\n",
    "def data_prep(df):\n",
    "    global n_input \n",
    "    global n_features\n",
    "    global n_output\n",
    "    global n_split\n",
    "\n",
    "    # Select features of use for later on\n",
    "    # df_input = df[['net_load', 'precipitation_probability',\n",
    "    #             'solar_radiation','sunshine_duration','weekend_or_bank_holiday', \n",
    "    #                 'month', 'day', 'hour', 'day_of_week', 'season', \n",
    "    #                 'avg_net_load',]]\n",
    "\n",
    "    df_input = df[['net_load', 'precipitation_probability',\n",
    "            'solar_radiation','sunshine_duration','weekend_or_bank_holiday']]\n",
    "    \n",
    "    \n",
    "    # variables = ['net_load','solar_radiation','sunshine_duration', 'precipitation_probability', 'avg_net_load' ]\n",
    "    variables = ['net_load','solar_radiation','sunshine_duration', 'precipitation_probability']\n",
    "\n",
    "    for var in variables:\n",
    "        new_var_name = var + '_norm'\n",
    "        df_input[new_var_name] = scaler.fit_transform(df_input[[var]])\n",
    "\n",
    "    df_input['sunshine_duration_norm(t+48)'] = df_input['sunshine_duration_norm'].shift(-48) #Add prediction for time the day after\n",
    "    df_input['solar_radiation_norm(t+48)'] = df_input['solar_radiation_norm'].shift(-48) #Add prediction for time the day after\n",
    "    df_input['precipitation_probability_norm(t+48)'] = df_input['precipitation_probability_norm'].shift(-48) #Add prediction for time the day after\n",
    "\n",
    "    df_input.dropna(inplace=True)\n",
    "\n",
    "    df_input = df_input[['net_load_norm', 'avg_net_load_norm', 'weekend_or_bank_holiday', 'sunshine_duration_norm(t+48)',\n",
    "               'solar_radiation_norm(t+48)','precipitation_probability_norm(t+48)',\n",
    "               'month', 'day', 'hour', 'day_of_week', 'season', \n",
    "                    ]]\n",
    "    \n",
    "    X, y = [], []\n",
    "    for i in range(len(df_input) - n_input - n_output + 1):\n",
    "        # Select all columns for the input sequence\n",
    "        X.append(df_input.iloc[i:(i + n_input)].values)  \n",
    "        \n",
    "        y.append(df_input['net_load_norm'].iloc[(i + n_input):(i + n_input + n_output)].values)\n",
    "\n",
    "    X = np.array(X).reshape((len(X), n_input, -1))  \n",
    "    y = np.array(y)\n",
    "\n",
    "    X_train = X[:n_split]\n",
    "    y_train = y[:n_split]\n",
    "\n",
    "    X_test = X[n_split:]\n",
    "    y_test = y[n_split:]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, df_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Fit LSTM/ GRU Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all Necessary Keras Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.losses import MeanAbsoluteError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def build_model(X_train, y_train, model_name):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True ))\n",
    "    model.add(BatchNormalization()) \n",
    "    model.add(LeakyReLU(alpha=0.01)) \n",
    "\n",
    "    model.add(LSTM(32))\n",
    "    model.add(BatchNormalization()) \n",
    "    model.add(LeakyReLU(alpha=0.01)) \n",
    "\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(y_train.shape[1]))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "    #Compile Model\n",
    "    cp1 = ModelCheckpoint(f'{model_name}.keras', save_best_only=True)\n",
    "    model.compile(loss='mse', optimizer=optimizer,  metrics=[MeanSquaredError(), MeanAbsoluteError()])\n",
    "\n",
    "    #Fit Model to Training Data with 10% Validation Split\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=16, validation_split=0.1, callbacks=[cp1])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Training Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_metrics(model, X_train, y_train):\n",
    "    _, train_mse, train_mae = model.evaluate(X_train, y_train, verbose=1)\n",
    "\n",
    "    return train_mse, train_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_viz(start_section, end_section, df_input, X_train, y_train):\n",
    "    global n_input \n",
    "    global n_features\n",
    "    global n_output\n",
    "    global n_split\n",
    "\n",
    "\n",
    "\n",
    "    df_ts_train = df_input[n_input:n_split]\n",
    "\n",
    "    X_train_shift = X_train[24:]\n",
    "    y_train_shift = y_train[24:]\n",
    "\n",
    "    X_train_input = X_train_shift[::48]\n",
    "    y_train_input = y_train_shift[::48]\n",
    "\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for i in X_train_input[start_section:end_section]:\n",
    "        i_reshaped = i.reshape(1, n_input, n_features)\n",
    "\n",
    "        prediction = model.predict(i_reshaped)\n",
    "\n",
    "        # We want to omit the first 12 hours the predictions i.e., the non-shaded squares in the figure above.\n",
    "        prediction_list = list(prediction[0][24:])\n",
    "        predictions.append(prediction_list)\n",
    "\n",
    "\n",
    "    y_train_input_24h = []\n",
    "\n",
    "    for i in y_train_input[start_section:end_section]:\n",
    "        # i_reshaped = i.reshape(1, n_input, n_features)\n",
    "        y_train_input_24h.append(i[24:])\n",
    "        \n",
    "\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.title(f\"Date Range: {df_ts_train.index[start_section*48+48].strftime('%d/%m/%Y')} - {df_ts_train.index[end_section*48+48].strftime('%d/%m/%Y')}\")\n",
    "\n",
    "    # We want to plot the predictions from 00:00 to 00:00 the next day.\n",
    "    plt.plot(df_ts_train.index[start_section*48+48: end_section*48+48], np.array(predictions).flatten(), label='Prediction')\n",
    "    plt.plot(df_ts_train.index[start_section*48+48: end_section*48+48], np.array(y_train_input_24h).flatten(), label = 'Actual')\n",
    "    plt.xlabel('Datetime')\n",
    "    plt.ylabel('Normalised Net Load (-)')\n",
    "\n",
    "\n",
    "    # Generate vertical lines for when we predict\n",
    "    xcoords_DAM = df_ts_train.index[start_section*48+72: (end_section)*48+72][::48]\n",
    "\n",
    "    for i, xc in enumerate(xcoords_DAM):\n",
    "        if i == 0:  # First item gets the label\n",
    "            plt.axvline(x=xc, color='red', linestyle='--', alpha=0.5, label='DAM Gate Closure')\n",
    "        else:  # Subsequent items do not\n",
    "            plt.axvline(x=xc, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "\n",
    "\n",
    "    # Generate vertical lines for DAM closure\n",
    "    xcoords_pred = df_ts_train.index[start_section*48+48: (end_section+1)*48+48][::48]\n",
    "\n",
    "    for i, xc in enumerate(xcoords_pred):\n",
    "        if i == 0:  # First item gets the label\n",
    "            plt.axvline(x=xc, color='purple', linestyle='--', alpha=0.3, label='Energy Delivery Prediction')\n",
    "        else:  # Subsequent items do not\n",
    "            plt.axvline(x=xc, color='purple', linestyle='--', alpha=0.3)\n",
    "\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(0.5, 1.15), loc='center', borderaxespad=0., ncol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Test Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def get_test_metrics(model, X_test, y_test):\n",
    "    global n_input \n",
    "    global n_features\n",
    "    global n_output\n",
    "    global n_split\n",
    "\n",
    "    X_test_shift = X_test[24:]\n",
    "    y_test_shift = y_test[24:]\n",
    "\n",
    "    X_test_input = X_test_shift[::48]\n",
    "    y_test_input = y_test_shift[::48]\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for i in X_test_input:\n",
    "        i_reshaped = i.reshape(1, n_input, n_features)\n",
    "\n",
    "        prediction = model.predict(i_reshaped)\n",
    "\n",
    "        # We want to omit the first 12 hours the predictions i.e., the non-shaded squares in the figure above.\n",
    "        prediction_list = list(prediction[0][24:])\n",
    "        predictions.append(prediction_list)\n",
    "\n",
    "\n",
    "    y_test_input_24h = []\n",
    "\n",
    "    for i in y_test_input:\n",
    "        # i_reshaped = i.reshape(1, n_input, n_features)\n",
    "        y_test_input_24h.append(i[24:])\n",
    "\n",
    "    # Flatten the lists of lists into single lists\n",
    "    predictions_flat = [item for sublist in predictions for item in sublist]\n",
    "    y_test_flat = [item for sublist in y_test_input_24h for item in sublist]\n",
    "\n",
    "    # Calculate MSE\n",
    "    test_mse = mean_squared_error(y_test_flat, predictions_flat)\n",
    "\n",
    "    # Calculate MAE\n",
    "    test_mae = mean_absolute_error(y_test_flat, predictions_flat)\n",
    "\n",
    "    return test_mse, test_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testing_viz(start_section, end_section, df_input, X_test, y_test):\n",
    "    global n_input \n",
    "    global n_features\n",
    "    global n_output\n",
    "    global n_split\n",
    "\n",
    "    df_ts_test = df_input[n_input+n_split:]\n",
    "\n",
    "    X_test_shift = X_test[24:]\n",
    "    y_test_shift = y_test[24:]\n",
    "\n",
    "    X_test_input = X_test_shift[::48]\n",
    "    y_test_input = y_test_shift[::48]\n",
    "\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for i in X_test_input[start_section:end_section]:\n",
    "        i_reshaped = i.reshape(1, n_input, n_features)\n",
    "\n",
    "        prediction = model.predict(i_reshaped)\n",
    "\n",
    "        # We want to omit the first 12 hours the predictions i.e., the non-shaded squares in the figure above.\n",
    "        prediction_list = list(prediction[0][24:])\n",
    "        predictions.append(prediction_list)\n",
    "\n",
    "\n",
    "    y_test_input_24h = []\n",
    "\n",
    "    for i in y_test_input[start_section:end_section]:\n",
    "        # i_reshaped = i.reshape(1, n_input, n_features)\n",
    "        y_test_input_24h.append(i[24:])\n",
    "        \n",
    "\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.title(f\"Date Range: {df_ts_test.index[start_section*48+48].strftime('%d/%m/%Y')} - {df_ts_test.index[end_section*48+48].strftime('%d/%m/%Y')}\")\n",
    "\n",
    "    # We want to plot the predictions from 00:00 to 00:00 the next day.\n",
    "    plt.plot(df_ts_test.index[start_section*48+48: end_section*48+48], np.array(predictions).flatten(), label='Prediction')\n",
    "    plt.plot(df_ts_test.index[start_section*48+48: end_section*48+48], np.array(y_test_input_24h).flatten(), label = 'Actual')\n",
    "    plt.xlabel('Datetime')\n",
    "    plt.ylabel('Normalised Net Load (-)')\n",
    "\n",
    "\n",
    "    # Generate vertical lines for when we predict\n",
    "    xcoords_DAM = df_ts_test.index[start_section*48+72: (end_section)*48+72][::48]\n",
    "\n",
    "    for i, xc in enumerate(xcoords_DAM):\n",
    "        if i == 0:  # First item gets the label\n",
    "            plt.axvline(x=xc, color='red', linestyle='--', alpha=0.5, label='DAM Gate Closure')\n",
    "        else:  # Subsequent items do not\n",
    "            plt.axvline(x=xc, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "\n",
    "\n",
    "    # Generate vertical lines for DAM closure\n",
    "    xcoords_pred = df_ts_test.index[start_section*48+48: (end_section+1)*48+48][::48]\n",
    "\n",
    "    for i, xc in enumerate(xcoords_pred):\n",
    "        if i == 0:  # First item gets the label\n",
    "            plt.axvline(x=xc, color='purple', linestyle='--', alpha=0.3, label='Energy Delivery Prediction')\n",
    "        else:  # Subsequent items do not\n",
    "            plt.axvline(x=xc, color='purple', linestyle='--', alpha=0.3)\n",
    "\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(0.5, 1.15), loc='center', borderaxespad=0., ncol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_household_data(household)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, df_input = data_prep(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(X_train, y_train, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mse, train_mae = get_train_metrics(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_section = 0\n",
    "end_section = 5\n",
    "\n",
    "get_training_viz(start_section, end_section, df_input, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse, test_mae = get_test_metrics(model, X_test, y_test)\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test MAE: {test_mae}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_section = 7\n",
    "end_section = 14\n",
    "\n",
    "get_testing_viz(start_section, end_section, df_input, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get MAE and MSE Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, train_mse, train_mae = model.evaluate(X_train, y_train, verbose=1)\n",
    "print(f\"Train MSE: {train_mse}\")\n",
    "print(f\"Train MAE: {train_mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to change to better visualise the training results\n",
    "\n",
    "# 1 = number of days i.e., 24 hours i.e., 48 time steps\n",
    "\n",
    "start_section = 0\n",
    "end_section = 5\n",
    "diff = end_section-start_section\n",
    "\n",
    "df_ts_train = df_input[n_input:n_split]\n",
    "\n",
    "X_train_shift = X_train[24:]\n",
    "y_train_shift = y_train[24:]\n",
    "\n",
    "X_train_input = X_train_shift[::48]\n",
    "y_train_input = y_train_shift[::48]\n",
    "\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i in X_train_input[start_section:end_section]:\n",
    "    i_reshaped = i.reshape(1, n_input, n_features)\n",
    "\n",
    "    prediction = model.predict(i_reshaped)\n",
    "\n",
    "     # We want to omit the first 12 hours the predictions i.e., the non-shaded squares in the figure above.\n",
    "    prediction_list = list(prediction[0][24:])\n",
    "    predictions.append(prediction_list)\n",
    "\n",
    "\n",
    "y_train_input_24h = []\n",
    "\n",
    "for i in y_train_input[start_section:end_section]:\n",
    "    # i_reshaped = i.reshape(1, n_input, n_features)\n",
    "    y_train_input_24h.append(i[24:])\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.title(f\"Date Range: {df_ts_train.index[start_section*48+48].strftime('%d/%m/%Y')} - {df_ts_train.index[end_section*48+48].strftime('%d/%m/%Y')}\")\n",
    "\n",
    "# We want to plot the predictions from 00:00 to 00:00 the next day.\n",
    "plt.plot(df_ts_train.index[start_section*48+48: end_section*48+48], np.array(predictions).flatten(), label='Prediction')\n",
    "plt.plot(df_ts_train.index[start_section*48+48: end_section*48+48], np.array(y_train_input_24h).flatten(), label = 'Actual')\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Normalised Net Load (-)')\n",
    "\n",
    "\n",
    "# Generate vertical lines for when we predict\n",
    "xcoords_DAM = df_ts_train.index[start_section*48+72: (end_section)*48+72][::48]\n",
    "\n",
    "for i, xc in enumerate(xcoords_DAM):\n",
    "    if i == 0:  # First item gets the label\n",
    "        plt.axvline(x=xc, color='red', linestyle='--', alpha=0.5, label='DAM Gate Closure')\n",
    "    else:  # Subsequent items do not\n",
    "        plt.axvline(x=xc, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "\n",
    "\n",
    "# Generate vertical lines for DAM closure\n",
    "xcoords_pred = df_ts_train.index[start_section*48+48: (end_section+1)*48+48][::48]\n",
    "\n",
    "for i, xc in enumerate(xcoords_pred):\n",
    "    if i == 0:  # First item gets the label\n",
    "        plt.axvline(x=xc, color='purple', linestyle='--', alpha=0.3, label='Energy Delivery Prediction')\n",
    "    else:  # Subsequent items do not\n",
    "        plt.axvline(x=xc, color='purple', linestyle='--', alpha=0.3)\n",
    "\n",
    "\n",
    "plt.legend(bbox_to_anchor=(0.5, 1.15), loc='center', borderaxespad=0., ncol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get MAE and MSE Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "X_test_shift = X_test[24:]\n",
    "y_test_shift = y_test[24:]\n",
    "\n",
    "X_test_input = X_test_shift[::48]\n",
    "y_test_input = y_test_shift[::48]\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i in X_test_input:\n",
    "    i_reshaped = i.reshape(1, n_input, n_features)\n",
    "\n",
    "    prediction = model.predict(i_reshaped)\n",
    "\n",
    "     # We want to omit the first 12 hours the predictions i.e., the non-shaded squares in the figure above.\n",
    "    prediction_list = list(prediction[0][24:])\n",
    "    predictions.append(prediction_list)\n",
    "\n",
    "\n",
    "y_test_input_24h = []\n",
    "\n",
    "for i in y_test_input:\n",
    "    # i_reshaped = i.reshape(1, n_input, n_features)\n",
    "    y_test_input_24h.append(i[24:])\n",
    "\n",
    "# Flatten the lists of lists into single lists\n",
    "predictions_flat = [item for sublist in predictions for item in sublist]\n",
    "y_test_flat = [item for sublist in y_test_input_24h for item in sublist]\n",
    "\n",
    "# Calculate MSE\n",
    "test_mse = mean_squared_error(y_test_flat, predictions_flat)\n",
    "\n",
    "# Calculate MAE\n",
    "test_mae = mean_absolute_error(y_test_flat, predictions_flat)\n",
    "\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to change to better visualise the training results\n",
    "\n",
    "# 1 = number of days i.e., 24 hours i.e., 48 time steps\n",
    "\n",
    "start_section = 7\n",
    "end_section = 14\n",
    "diff = end_section-start_section\n",
    "\n",
    "df_ts_test = df_input[n_input+n_split:]\n",
    "\n",
    "X_test_shift = X_test[24:]\n",
    "y_test_shift = y_test[24:]\n",
    "\n",
    "X_test_input = X_test_shift[::48]\n",
    "y_test_input = y_test_shift[::48]\n",
    "\n",
    "\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i in X_test_input[start_section:end_section]:\n",
    "    i_reshaped = i.reshape(1, n_input, n_features)\n",
    "\n",
    "    prediction = model.predict(i_reshaped)\n",
    "\n",
    "     # We want to omit the first 12 hours the predictions i.e., the non-shaded squares in the figure above.\n",
    "    prediction_list = list(prediction[0][24:])\n",
    "    predictions.append(prediction_list)\n",
    "\n",
    "\n",
    "y_test_input_24h = []\n",
    "\n",
    "for i in y_test_input[start_section:end_section]:\n",
    "    # i_reshaped = i.reshape(1, n_input, n_features)\n",
    "    y_test_input_24h.append(i[24:])\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.title(f\"Date Range: {df_ts_test.index[start_section*48+48].strftime('%d/%m/%Y')} - {df_ts_test.index[end_section*48+48].strftime('%d/%m/%Y')}\")\n",
    "\n",
    "# We want to plot the predictions from 00:00 to 00:00 the next day.\n",
    "plt.plot(df_ts_test.index[start_section*48+48: end_section*48+48], np.array(predictions).flatten(), label='Prediction')\n",
    "plt.plot(df_ts_test.index[start_section*48+48: end_section*48+48], np.array(y_test_input_24h).flatten(), label = 'Actual')\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Normalised Net Load (-)')\n",
    "\n",
    "\n",
    "# Generate vertical lines for when we predict\n",
    "xcoords_DAM = df_ts_test.index[start_section*48+72: (end_section)*48+72][::48]\n",
    "\n",
    "for i, xc in enumerate(xcoords_DAM):\n",
    "    if i == 0:  # First item gets the label\n",
    "        plt.axvline(x=xc, color='red', linestyle='--', alpha=0.5, label='DAM Gate Closure')\n",
    "    else:  # Subsequent items do not\n",
    "        plt.axvline(x=xc, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "\n",
    "\n",
    "# Generate vertical lines for DAM closure\n",
    "xcoords_pred = df_ts_test.index[start_section*48+48: (end_section+1)*48+48][::48]\n",
    "\n",
    "for i, xc in enumerate(xcoords_pred):\n",
    "    if i == 0:  # First item gets the label\n",
    "        plt.axvline(x=xc, color='purple', linestyle='--', alpha=0.3, label='Energy Delivery Prediction')\n",
    "    else:  # Subsequent items do not\n",
    "        plt.axvline(x=xc, color='purple', linestyle='--', alpha=0.3)\n",
    "\n",
    "\n",
    "plt.legend(bbox_to_anchor=(0.5, 1.15), loc='center', borderaxespad=0., ncol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Search - Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters to tune:\n",
    "* \\# of LSTM/GRU units\n",
    "* \\# of LSTM/GRU layers\n",
    "* Dropout Rate\n",
    "* Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import MeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from kerastuner.tuners import RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(hp):\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(hp.Int('units_l1', min_value=64, max_value=256, step=64),\n",
    "#                    input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "#                    return_sequences=hp.Choice('return_seq', [True, False])))\n",
    "#     model.add(BatchNormalization()) \n",
    "#     model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "#     for i in range(hp.Int('num_layers', 1, 3)):\n",
    "#         model.add(LSTM(hp.Int(f'units_l{i+2}', min_value=64, max_value=256, step=64), return_sequences=False if i==hp.get('num_layers')-1 else True))\n",
    "#         model.add(BatchNormalization()) \n",
    "#         model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "#     model.add(Dropout(hp.Float('dropout', min_value=0.0, max_value=0.3, step=0.1)))\n",
    "#     model.add(Dense(y_train.shape[1]))\n",
    "\n",
    "#     model.compile(optimizer=Adam(hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
    "#                   loss='mse',\n",
    "#                   metrics=[MeanSquaredError(), MeanAbsoluteError()])\n",
    "\n",
    "#     return model\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    # Ensure the first LSTM layer always returns sequences\n",
    "    model.add(LSTM(hp.Int('units_l1', min_value=64, max_value=256, step=64),\n",
    "                   input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   return_sequences=True))  # Always return sequences for the first LSTM layer\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "    # Adjust subsequent LSTM layers based on their position\n",
    "    for i in range(1, hp.Int('num_layers', 1, 3)):  # Start the range from 1 since we've already added the first LSTM layer\n",
    "        model.add(LSTM(hp.Int(f'units_l{i+1}', min_value=64, max_value=256, step=64), \n",
    "                       return_sequences=False if i == hp.get('num_layers')-1 else True))\n",
    "        model.add(BatchNormalization()) \n",
    "        model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "    model.add(Dropout(hp.Float('dropout', min_value=0.0, max_value=0.3, step=0.1)))\n",
    "    model.add(Dense(y_train.shape[1]))\n",
    "\n",
    "    model.compile(optimizer=Adam(hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
    "                  loss='mse',\n",
    "                  metrics=[MeanSquaredError(), MeanAbsoluteError()])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the tuner\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='hyperparam_tuning',\n",
    "    project_name='house20'\n",
    ")\n",
    "\n",
    "# Display search space summary\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing the hyperparameter search\n",
    "tuner.search(X_train, y_train,\n",
    "             epochs=10,\n",
    "             batch_size=16,\n",
    "             validation_split=0.1,\n",
    "             callbacks=[ModelCheckpoint('model_{epoch}.keras', save_best_only=True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The optimal number of units in the first LSTM layer is {best_hps.get('units_first_layer')} and the second LSTM layer is {best_hps.get('units_second_layer')}.\n",
    "The optimal dropout rate is {best_hps.get('dropout')}.\n",
    "The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with the best hyperparameters and train it on the data for 20 epochs\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_split=0.1, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model building function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras_tuner import RandomSearch\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    # model.add(LSTM(hp.Choice('num_lstm_units_first_layer', values=[32, 64, 128, 256]),\n",
    "    #                input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "    #                return_sequences=True if hp.Int('num_lstm_layers', 1, 3) > 1 else False))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "    model.add(LSTM(hp.Choice('num_lstm_units_first_layer', values=[8,4]),\n",
    "                   input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   return_sequences=True if hp.Int('num_lstm_layers', 1, 3) > 1 else False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "    # num_lstm_layers = hp.Int('num_lstm_layers', 1, 3)\n",
    "    # for i in range(num_lstm_layers - 1):\n",
    "    #     model.add(LSTM(hp.Choice(f'num_lstm_units_layer_{i+2}', values=[32, 64, 128, 256]),\n",
    "    #                    return_sequences=(i < num_lstm_layers - 2)))  # Only return sequences if not the last LSTM layer\n",
    "    #     model.add(BatchNormalization())\n",
    "    #     model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "    num_lstm_layers = hp.Int('num_lstm_layers', 1, 3)\n",
    "    for i in range(num_lstm_layers - 1):\n",
    "        model.add(LSTM(hp.Choice(f'num_lstm_units_layer_{i+2}', values=[4,2]),\n",
    "                       return_sequences=(i < num_lstm_layers - 2)))  # Only return sequences if not the last LSTM layer\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "\n",
    "\n",
    "    model.add(Dropout(hp.Float('dropout_rate', min_value=0.0, max_value=0.2, step=0.1)))\n",
    "    model.add(Dense(y_train.shape[1]))\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                  optimizer=Adam(learning_rate=0.001),\n",
    "                  metrics=[MeanSquaredError(), MeanAbsoluteError()])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiliase random search tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=5,  # Number of trials to run\n",
    "    executions_per_trial=1,  # Number of models to build for each trial\n",
    "    directory='keras_tuner_dir_LSTM_2',  # Directory to save logs and models\n",
    "    project_name='lstm_tuning'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(X_train, y_train,\n",
    "             epochs=10,\n",
    "             validation_split=0.1,\n",
    "             callbacks=[ModelCheckpoint('final_model.keras', save_best_only=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarise results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a summary of the search\n",
    "tuner.results_summary()\n",
    "\n",
    "# Retrieve the best model's hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"The best hyperparameters are:\")\n",
    "print(f\"- Number of units in the first LSTM layer: {best_hps.get('num_lstm_units_first_layer')}\")\n",
    "for i in range(1, best_hps.get('num_lstm_layers')):\n",
    "    print(f\"- Number of units in LSTM layer {i+1}: {best_hps.get(f'num_lstm_units_layer_{i+2}')}\")\n",
    "print(f\"- Dropout rate: {best_hps.get('dropout_rate')}\")\n",
    "print(f\"- Number of LSTM layers: {best_hps.get('num_lstm_layers')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "X_test_shift = X_test[24:]\n",
    "y_test_shift = y_test[24:]\n",
    "\n",
    "X_test_input = X_test_shift[::48]\n",
    "y_test_input = y_test_shift[::48]\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i in X_test_input:\n",
    "    i_reshaped = i.reshape(1, n_input, n_features)\n",
    "\n",
    "    prediction = model.predict(i_reshaped)\n",
    "\n",
    "     # We want to omit the first 12 hours the predictions i.e., the non-shaded squares in the figure above.\n",
    "    prediction_list = list(prediction[0][24:])\n",
    "    predictions.append(prediction_list)\n",
    "\n",
    "\n",
    "y_test_input_24h = []\n",
    "\n",
    "for i in y_test_input:\n",
    "    # i_reshaped = i.reshape(1, n_input, n_features)\n",
    "    y_test_input_24h.append(i[24:])\n",
    "\n",
    "# Flatten the lists of lists into single lists\n",
    "predictions_flat = [item for sublist in predictions for item in sublist]\n",
    "y_test_flat = [item for sublist in y_test_input_24h for item in sublist]\n",
    "\n",
    "# Calculate MSE\n",
    "test_mse = mean_squared_error(y_test_flat, predictions_flat)\n",
    "\n",
    "# Calculate MAE\n",
    "test_mae = mean_absolute_error(y_test_flat, predictions_flat)\n",
    "\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test MAE: {test_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to change to better visualise the training results\n",
    "\n",
    "# 1 = number of days i.e., 24 hours i.e., 48 time steps\n",
    "\n",
    "start_section = 7\n",
    "end_section = 14\n",
    "diff = end_section-start_section\n",
    "\n",
    "df_ts_test = df_input[n_input+n_split:]\n",
    "\n",
    "X_test_shift = X_test[24:]\n",
    "y_test_shift = y_test[24:]\n",
    "\n",
    "X_test_input = X_test_shift[::48]\n",
    "y_test_input = y_test_shift[::48]\n",
    "\n",
    "\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i in X_test_input[start_section:end_section]:\n",
    "    i_reshaped = i.reshape(1, n_input, n_features)\n",
    "\n",
    "    prediction = model.predict(i_reshaped)\n",
    "\n",
    "     # We want to omit the first 12 hours the predictions i.e., the non-shaded squares in the figure above.\n",
    "    prediction_list = list(prediction[0][24:])\n",
    "    predictions.append(prediction_list)\n",
    "\n",
    "\n",
    "y_test_input_24h = []\n",
    "\n",
    "for i in y_test_input[start_section:end_section]:\n",
    "    # i_reshaped = i.reshape(1, n_input, n_features)\n",
    "    y_test_input_24h.append(i[24:])\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.title(f\"Date Range: {df_ts_test.index[start_section*48+48].strftime('%d/%m/%Y')} - {df_ts_test.index[end_section*48+48].strftime('%d/%m/%Y')}\")\n",
    "\n",
    "# We want to plot the predictions from 00:00 to 00:00 the next day.\n",
    "plt.plot(df_ts_test.index[start_section*48+48: end_section*48+48], np.array(predictions).flatten(), label='Prediction')\n",
    "plt.plot(df_ts_test.index[start_section*48+48: end_section*48+48], np.array(y_test_input_24h).flatten(), label = 'Actual')\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Normalised Net Load (-)')\n",
    "\n",
    "\n",
    "# Generate vertical lines for when we predict\n",
    "xcoords_DAM = df_ts_test.index[start_section*48+72: (end_section)*48+72][::48]\n",
    "\n",
    "for i, xc in enumerate(xcoords_DAM):\n",
    "    if i == 0:  # First item gets the label\n",
    "        plt.axvline(x=xc, color='red', linestyle='--', alpha=0.5, label='DAM Gate Closure')\n",
    "    else:  # Subsequent items do not\n",
    "        plt.axvline(x=xc, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "\n",
    "\n",
    "# Generate vertical lines for DAM closure\n",
    "xcoords_pred = df_ts_test.index[start_section*48+48: (end_section+1)*48+48][::48]\n",
    "\n",
    "for i, xc in enumerate(xcoords_pred):\n",
    "    if i == 0:  # First item gets the label\n",
    "        plt.axvline(x=xc, color='purple', linestyle='--', alpha=0.3, label='Energy Delivery Prediction')\n",
    "    else:  # Subsequent items do not\n",
    "        plt.axvline(x=xc, color='purple', linestyle='--', alpha=0.3)\n",
    "\n",
    "\n",
    "plt.legend(bbox_to_anchor=(0.5, 1.15), loc='center', borderaxespad=0., ncol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "build_model(kt.HyperParameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtuner = kt.RandomSearch(hypermodel=build_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=5,\n",
    "    seed=12345,\n",
    "    overwrite=True,\n",
    "    directory=\"my_dir\",\n",
    "    project_name=\"diab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtuner.search(X_train,\n",
    "              y_train,\n",
    "              epochs=10,\n",
    "              validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model = rtuner.get_best_models(num_models=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtuner.get_best_hyperparameters().get_config()['values']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
