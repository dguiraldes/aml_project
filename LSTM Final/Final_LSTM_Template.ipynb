{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><u> Final LSTM/GRU Layout - Adri and Elian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Libraries and Packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Access Credentials\n",
    "db_host = os.getenv(\"DB_HOST\")\n",
    "db_name = os.getenv(\"DB_NAME\")\n",
    "db_user = os.getenv(\"DB_USER\")\n",
    "db_password = os.getenv(\"DB_PASSWORD\")\n",
    "db_port = os.getenv(\"DB_PORT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\n",
    "    host=db_host,\n",
    "    dbname=db_name,\n",
    "    user=db_user,\n",
    "    password=db_password,\n",
    "    port=db_port\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\"\n",
    "    select * \n",
    "    from agg.tidy_data_final\n",
    "    where site = 20\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elian\\AppData\\Local\\Temp\\ipykernel_99556\\1553155693.py:1: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('timestamp', drop=False)\n",
    "df = df.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data Set for LSTM/GRU Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "#n_input = no. of time steps considered before making the prediction\n",
    "#n_features = no. of variables/features considered\n",
    "#n_output = no. of time steps to forecast\n",
    "\n",
    "# Note: 1 time step = 30 mins\n",
    "# Look at the last two weeks to forecast the next 36 hours\n",
    "\n",
    "# 1 day = 2 * 24 * 1 = 24\n",
    "# 2 hours = 2 * 2 = 8 \n",
    "\n",
    "n_input = 144\n",
    "n_features = 11 #These includes the weather predictions that will be added later on and the normalised varaibles to be created\n",
    "n_output = 72\n",
    "\n",
    "\n",
    "# Select features of use for later on\n",
    "df_input = df[['net_load', 'precipitation_probability',\n",
    "               'solar_radiation','sunshine_duration','weekend_or_bank_holiday', \n",
    "                'month', 'day', 'hour', 'day_of_week', 'season', \n",
    "                'avg_net_load',]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalise Data (as seen in Literature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group decision to use MaxAbs Scaler to normalise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elian\\AppData\\Local\\Temp\\ipykernel_99556\\3894627056.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_input[new_var_name] = scaler.fit_transform(df_input[[var]])\n",
      "C:\\Users\\elian\\AppData\\Local\\Temp\\ipykernel_99556\\3894627056.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_input[new_var_name] = scaler.fit_transform(df_input[[var]])\n",
      "C:\\Users\\elian\\AppData\\Local\\Temp\\ipykernel_99556\\3894627056.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_input[new_var_name] = scaler.fit_transform(df_input[[var]])\n",
      "C:\\Users\\elian\\AppData\\Local\\Temp\\ipykernel_99556\\3894627056.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_input[new_var_name] = scaler.fit_transform(df_input[[var]])\n",
      "C:\\Users\\elian\\AppData\\Local\\Temp\\ipykernel_99556\\3894627056.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_input[new_var_name] = scaler.fit_transform(df_input[[var]])\n"
     ]
    }
   ],
   "source": [
    "#Normalise all continuous variables needing scaling\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "scaler = MaxAbsScaler()\n",
    "\n",
    "variables = ['net_load','solar_radiation','sunshine_duration', 'precipitation_probability', 'avg_net_load' ]\n",
    "for var in variables:\n",
    "    new_var_name = var + '_norm'\n",
    "    df_input[new_var_name] = scaler.fit_transform(df_input[[var]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we also want to include the predictions for 'solar radiation', 'sunshine_duration', 'precipitation_probability' as inputs.\n",
    "These will be treated as new inputs (t+48) ~ 48 half-hourly timestamps representing day-ahead predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elian\\AppData\\Local\\Temp\\ipykernel_99556\\1327219898.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_input['sunshine_duration_norm(t+48)'] = df_input['sunshine_duration_norm'].shift(-48) #Add prediction for time the day after\n",
      "C:\\Users\\elian\\AppData\\Local\\Temp\\ipykernel_99556\\1327219898.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_input['solar_radiation_norm(t+48)'] = df_input['solar_radiation_norm'].shift(-48) #Add prediction for time the day after\n",
      "C:\\Users\\elian\\AppData\\Local\\Temp\\ipykernel_99556\\1327219898.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_input['precipitation_probability_norm(t+48)'] = df_input['precipitation_probability_norm'].shift(-48) #Add prediction for time the day after\n"
     ]
    }
   ],
   "source": [
    "df_input['sunshine_duration_norm(t+48)'] = df_input['sunshine_duration_norm'].shift(-48) #Add prediction for time the day after\n",
    "df_input['solar_radiation_norm(t+48)'] = df_input['solar_radiation_norm'].shift(-48) #Add prediction for time the day after\n",
    "df_input['precipitation_probability_norm(t+48)'] = df_input['precipitation_probability_norm'].shift(-48) #Add prediction for time the day after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elian\\AppData\\Local\\Temp\\ipykernel_99556\\1212278532.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_input.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#We want to drop last 48 rows as they will have Na values for the (t+48) columns/inputs.\n",
    "df_input.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only Keep the Normalised Variables for the Model\n",
    "df_input = df_input[['net_load_norm', 'avg_net_load_norm', 'weekend_or_bank_holiday', 'sunshine_duration_norm(t+48)',\n",
    "               'solar_radiation_norm(t+48)','precipitation_probability_norm(t+48)',\n",
    "               'month', 'day', 'hour', 'day_of_week', 'season', \n",
    "                    ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate X and Y Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "for i in range(len(df_input) - n_input - n_output + 1):\n",
    "    # Select all columns for the input sequence\n",
    "    X.append(df_input.iloc[i:(i + n_input)].values)  \n",
    "    \n",
    "    y.append(df_input['net_load_norm'].iloc[(i + n_input):(i + n_input + n_output)].values)\n",
    "\n",
    "X = np.array(X).reshape((len(X), n_input, -1))  \n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 time step = 30 mins\n",
    "\n",
    "n_split = 2 * 24 * 324\n",
    "\n",
    "X_train = X[:n_split]\n",
    "y_train = y[:n_split]\n",
    "\n",
    "X_test = X[n_split:]\n",
    "y_test = y[n_split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Fit LSTM/ GRU Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all Necessary Keras Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.losses import MeanAbsoluteError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRU Model - just change GRU for LSTM to retrain\n",
    "#Model Architecture\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "model.add(LeakyReLU(alpha=0.01)) \n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LeakyReLU(alpha=0.01)) \n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(64))\n",
    "model.add(LeakyReLU(alpha=0.01)) \n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True ))\n",
    "model.add(BatchNormalization()) \n",
    "model.add(LeakyReLU(alpha=0.01)) \n",
    "\n",
    "model.add(LSTM(32))\n",
    "model.add(BatchNormalization()) \n",
    "model.add(LeakyReLU(alpha=0.01)) \n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_train.shape[1]))\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile Model\n",
    "cp1 = ModelCheckpoint('final_model.keras', save_best_only=True)\n",
    "model.compile(loss='mse', optimizer=optimizer,  metrics=[MeanSquaredError(), MeanAbsoluteError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit Model to Training Data with 10% Validation Split\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=16, validation_split=0.1, callbacks=[cp1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Outputs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get MAE and MSE Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, train_mse, train_mae = model.evaluate(X_train, y_train, verbose=1)\n",
    "print(f\"Train MSE: {train_mse}\")\n",
    "print(f\"Train MAE: {train_mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to change to better visualise the training results\n",
    "\n",
    "# 1 = number of days i.e., 24 hours i.e., 48 time steps\n",
    "\n",
    "start_section = 0\n",
    "end_section = 5\n",
    "diff = end_section-start_section\n",
    "\n",
    "df_ts_train = df_input[n_input:n_split]\n",
    "\n",
    "X_train_shift = X_train[24:]\n",
    "y_train_shift = y_train[24:]\n",
    "\n",
    "X_train_input = X_train_shift[::48]\n",
    "y_train_input = y_train_shift[::48]\n",
    "\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i in X_train_input[start_section:end_section]:\n",
    "    i_reshaped = i.reshape(1, n_input, n_features)\n",
    "\n",
    "    prediction = model.predict(i_reshaped)\n",
    "\n",
    "     # We want to omit the first 12 hours the predictions i.e., the non-shaded squares in the figure above.\n",
    "    prediction_list = list(prediction[0][24:])\n",
    "    predictions.append(prediction_list)\n",
    "\n",
    "\n",
    "y_train_input_24h = []\n",
    "\n",
    "for i in y_train_input[start_section:end_section]:\n",
    "    # i_reshaped = i.reshape(1, n_input, n_features)\n",
    "    y_train_input_24h.append(i[24:])\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.title(f\"Date Range: {df_ts_train.index[start_section*48+48].strftime('%d/%m/%Y')} - {df_ts_train.index[end_section*48+48].strftime('%d/%m/%Y')}\")\n",
    "\n",
    "# We want to plot the predictions from 00:00 to 00:00 the next day.\n",
    "plt.plot(df_ts_train.index[start_section*48+48: end_section*48+48], np.array(predictions).flatten(), label='Prediction')\n",
    "plt.plot(df_ts_train.index[start_section*48+48: end_section*48+48], np.array(y_train_input_24h).flatten(), label = 'Actual')\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Normalised Net Load (-)')\n",
    "\n",
    "\n",
    "# Generate vertical lines for when we predict\n",
    "xcoords_DAM = df_ts_train.index[start_section*48+72: (end_section)*48+72][::48]\n",
    "\n",
    "for i, xc in enumerate(xcoords_DAM):\n",
    "    if i == 0:  # First item gets the label\n",
    "        plt.axvline(x=xc, color='red', linestyle='--', alpha=0.5, label='DAM Gate Closure')\n",
    "    else:  # Subsequent items do not\n",
    "        plt.axvline(x=xc, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "\n",
    "\n",
    "# Generate vertical lines for DAM closure\n",
    "xcoords_pred = df_ts_train.index[start_section*48+48: (end_section+1)*48+48][::48]\n",
    "\n",
    "for i, xc in enumerate(xcoords_pred):\n",
    "    if i == 0:  # First item gets the label\n",
    "        plt.axvline(x=xc, color='purple', linestyle='--', alpha=0.3, label='Energy Delivery Prediction')\n",
    "    else:  # Subsequent items do not\n",
    "        plt.axvline(x=xc, color='purple', linestyle='--', alpha=0.3)\n",
    "\n",
    "\n",
    "plt.legend(bbox_to_anchor=(0.5, 1.15), loc='center', borderaxespad=0., ncol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get MAE and MSE Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "X_test_shift = X_test[24:]\n",
    "y_test_shift = y_test[24:]\n",
    "\n",
    "X_test_input = X_test_shift[::48]\n",
    "y_test_input = y_test_shift[::48]\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i in X_test_input:\n",
    "    i_reshaped = i.reshape(1, n_input, n_features)\n",
    "\n",
    "    prediction = model.predict(i_reshaped)\n",
    "\n",
    "     # We want to omit the first 12 hours the predictions i.e., the non-shaded squares in the figure above.\n",
    "    prediction_list = list(prediction[0][24:])\n",
    "    predictions.append(prediction_list)\n",
    "\n",
    "\n",
    "y_test_input_24h = []\n",
    "\n",
    "for i in y_test_input:\n",
    "    # i_reshaped = i.reshape(1, n_input, n_features)\n",
    "    y_test_input_24h.append(i[24:])\n",
    "\n",
    "# Flatten the lists of lists into single lists\n",
    "predictions_flat = [item for sublist in predictions for item in sublist]\n",
    "y_test_flat = [item for sublist in y_test_input_24h for item in sublist]\n",
    "\n",
    "# Calculate MSE\n",
    "test_mse = mean_squared_error(y_test_flat, predictions_flat)\n",
    "\n",
    "# Calculate MAE\n",
    "test_mae = mean_absolute_error(y_test_flat, predictions_flat)\n",
    "\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to change to better visualise the training results\n",
    "\n",
    "# 1 = number of days i.e., 24 hours i.e., 48 time steps\n",
    "\n",
    "start_section = 7\n",
    "end_section = 14\n",
    "diff = end_section-start_section\n",
    "\n",
    "df_ts_test = df_input[n_input+n_split:]\n",
    "\n",
    "X_test_shift = X_test[24:]\n",
    "y_test_shift = y_test[24:]\n",
    "\n",
    "X_test_input = X_test_shift[::48]\n",
    "y_test_input = y_test_shift[::48]\n",
    "\n",
    "\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i in X_test_input[start_section:end_section]:\n",
    "    i_reshaped = i.reshape(1, n_input, n_features)\n",
    "\n",
    "    prediction = model.predict(i_reshaped)\n",
    "\n",
    "     # We want to omit the first 12 hours the predictions i.e., the non-shaded squares in the figure above.\n",
    "    prediction_list = list(prediction[0][24:])\n",
    "    predictions.append(prediction_list)\n",
    "\n",
    "\n",
    "y_test_input_24h = []\n",
    "\n",
    "for i in y_test_input[start_section:end_section]:\n",
    "    # i_reshaped = i.reshape(1, n_input, n_features)\n",
    "    y_test_input_24h.append(i[24:])\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.title(f\"Date Range: {df_ts_test.index[start_section*48+48].strftime('%d/%m/%Y')} - {df_ts_test.index[end_section*48+48].strftime('%d/%m/%Y')}\")\n",
    "\n",
    "# We want to plot the predictions from 00:00 to 00:00 the next day.\n",
    "plt.plot(df_ts_test.index[start_section*48+48: end_section*48+48], np.array(predictions).flatten(), label='Prediction')\n",
    "plt.plot(df_ts_test.index[start_section*48+48: end_section*48+48], np.array(y_test_input_24h).flatten(), label = 'Actual')\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Normalised Net Load (-)')\n",
    "\n",
    "\n",
    "# Generate vertical lines for when we predict\n",
    "xcoords_DAM = df_ts_test.index[start_section*48+72: (end_section)*48+72][::48]\n",
    "\n",
    "for i, xc in enumerate(xcoords_DAM):\n",
    "    if i == 0:  # First item gets the label\n",
    "        plt.axvline(x=xc, color='red', linestyle='--', alpha=0.5, label='DAM Gate Closure')\n",
    "    else:  # Subsequent items do not\n",
    "        plt.axvline(x=xc, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "\n",
    "\n",
    "# Generate vertical lines for DAM closure\n",
    "xcoords_pred = df_ts_test.index[start_section*48+48: (end_section+1)*48+48][::48]\n",
    "\n",
    "for i, xc in enumerate(xcoords_pred):\n",
    "    if i == 0:  # First item gets the label\n",
    "        plt.axvline(x=xc, color='purple', linestyle='--', alpha=0.3, label='Energy Delivery Prediction')\n",
    "    else:  # Subsequent items do not\n",
    "        plt.axvline(x=xc, color='purple', linestyle='--', alpha=0.3)\n",
    "\n",
    "\n",
    "plt.legend(bbox_to_anchor=(0.5, 1.15), loc='center', borderaxespad=0., ncol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Search - Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters to tune:\n",
    "* \\# of LSTM/GRU units\n",
    "* \\# of LSTM/GRU layers\n",
    "* Dropout Rate\n",
    "* Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elian\\AppData\\Local\\Temp\\ipykernel_99556\\4037483222.py:6: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import RandomSearch\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import MeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from kerastuner.tuners import RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(hp):\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(hp.Int('units_l1', min_value=64, max_value=256, step=64),\n",
    "#                    input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "#                    return_sequences=hp.Choice('return_seq', [True, False])))\n",
    "#     model.add(BatchNormalization()) \n",
    "#     model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "#     for i in range(hp.Int('num_layers', 1, 3)):\n",
    "#         model.add(LSTM(hp.Int(f'units_l{i+2}', min_value=64, max_value=256, step=64), return_sequences=False if i==hp.get('num_layers')-1 else True))\n",
    "#         model.add(BatchNormalization()) \n",
    "#         model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "#     model.add(Dropout(hp.Float('dropout', min_value=0.0, max_value=0.3, step=0.1)))\n",
    "#     model.add(Dense(y_train.shape[1]))\n",
    "\n",
    "#     model.compile(optimizer=Adam(hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
    "#                   loss='mse',\n",
    "#                   metrics=[MeanSquaredError(), MeanAbsoluteError()])\n",
    "\n",
    "#     return model\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    # Ensure the first LSTM layer always returns sequences\n",
    "    model.add(LSTM(hp.Int('units_l1', min_value=64, max_value=256, step=64),\n",
    "                   input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   return_sequences=True))  # Always return sequences for the first LSTM layer\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "    # Adjust subsequent LSTM layers based on their position\n",
    "    for i in range(1, hp.Int('num_layers', 1, 3)):  # Start the range from 1 since we've already added the first LSTM layer\n",
    "        model.add(LSTM(hp.Int(f'units_l{i+1}', min_value=64, max_value=256, step=64), \n",
    "                       return_sequences=False if i == hp.get('num_layers')-1 else True))\n",
    "        model.add(BatchNormalization()) \n",
    "        model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "    model.add(Dropout(hp.Float('dropout', min_value=0.0, max_value=0.3, step=0.1)))\n",
    "    model.add(Dense(y_train.shape[1]))\n",
    "\n",
    "    model.compile(optimizer=Adam(hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
    "                  loss='mse',\n",
    "                  metrics=[MeanSquaredError(), MeanAbsoluteError()])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from hyperparam_tuning\\house20\\tuner0.json\n",
      "Search space summary\n",
      "Default search space size: 6\n",
      "units_l1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 256, 'step': 64, 'sampling': 'linear'}\n",
      "return_seq (Choice)\n",
      "{'default': 1, 'conditions': [], 'values': [1, 0], 'ordered': True}\n",
      "num_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 3, 'step': 1, 'sampling': 'linear'}\n",
      "units_l2 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 256, 'step': 64, 'sampling': 'linear'}\n",
      "dropout (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.3, 'step': 0.1, 'sampling': 'linear'}\n",
      "learning_rate (Choice)\n",
      "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.001, 0.0001], 'ordered': True}\n"
     ]
    }
   ],
   "source": [
    "# Setting up the tuner\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='hyperparam_tuning',\n",
    "    project_name='house20'\n",
    ")\n",
    "\n",
    "# Display search space summary\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 28m 42s]\n",
      "val_loss: 0.010563707910478115\n",
      "\n",
      "Best val_loss So Far: 0.009851577691733837\n",
      "Total elapsed time: 01h 57m 15s\n"
     ]
    }
   ],
   "source": [
    "# Performing the hyperparameter search\n",
    "tuner.search(X_train, y_train,\n",
    "             epochs=10,\n",
    "             batch_size=16,\n",
    "             validation_split=0.1,\n",
    "             callbacks=[ModelCheckpoint('model_{epoch}.keras', save_best_only=True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The optimal number of units in the first LSTM layer is {best_hps.get('units_first_layer')} and the second LSTM layer is {best_hps.get('units_second_layer')}.\n",
    "The optimal dropout rate is {best_hps.get('dropout')}.\n",
    "The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with the best hyperparameters and train it on the data for 20 epochs\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_split=0.1, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model building function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras_tuner import RandomSearch\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    # model.add(LSTM(hp.Choice('num_lstm_units_first_layer', values=[32, 64, 128, 256]),\n",
    "    #                input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "    #                return_sequences=True if hp.Int('num_lstm_layers', 1, 3) > 1 else False))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "    model.add(LSTM(hp.Choice('num_lstm_units_first_layer', values=[8,4]),\n",
    "                   input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   return_sequences=True if hp.Int('num_lstm_layers', 1, 3) > 1 else False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "    # num_lstm_layers = hp.Int('num_lstm_layers', 1, 3)\n",
    "    # for i in range(num_lstm_layers - 1):\n",
    "    #     model.add(LSTM(hp.Choice(f'num_lstm_units_layer_{i+2}', values=[32, 64, 128, 256]),\n",
    "    #                    return_sequences=(i < num_lstm_layers - 2)))  # Only return sequences if not the last LSTM layer\n",
    "    #     model.add(BatchNormalization())\n",
    "    #     model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "    num_lstm_layers = hp.Int('num_lstm_layers', 1, 3)\n",
    "    for i in range(num_lstm_layers - 1):\n",
    "        model.add(LSTM(hp.Choice(f'num_lstm_units_layer_{i+2}', values=[4,2]),\n",
    "                       return_sequences=(i < num_lstm_layers - 2)))  # Only return sequences if not the last LSTM layer\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "\n",
    "\n",
    "    model.add(Dropout(hp.Float('dropout_rate', min_value=0.0, max_value=0.2, step=0.1)))\n",
    "    model.add(Dense(y_train.shape[1]))\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                  optimizer=Adam(learning_rate=0.001),\n",
    "                  metrics=[MeanSquaredError(), MeanAbsoluteError()])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiliase random search tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=5,  # Number of trials to run\n",
    "    executions_per_trial=1,  # Number of models to build for each trial\n",
    "    directory='keras_tuner_dir_LSTM_2',  # Directory to save logs and models\n",
    "    project_name='lstm_tuning'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(X_train, y_train,\n",
    "             epochs=10,\n",
    "             validation_split=0.1,\n",
    "             callbacks=[ModelCheckpoint('final_model.keras', save_best_only=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarise results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a summary of the search\n",
    "tuner.results_summary()\n",
    "\n",
    "# Retrieve the best model's hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"The best hyperparameters are:\")\n",
    "print(f\"- Number of units in the first LSTM layer: {best_hps.get('num_lstm_units_first_layer')}\")\n",
    "for i in range(1, best_hps.get('num_lstm_layers')):\n",
    "    print(f\"- Number of units in LSTM layer {i+1}: {best_hps.get(f'num_lstm_units_layer_{i+2}')}\")\n",
    "print(f\"- Dropout rate: {best_hps.get('dropout_rate')}\")\n",
    "print(f\"- Number of LSTM layers: {best_hps.get('num_lstm_layers')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "X_test_shift = X_test[24:]\n",
    "y_test_shift = y_test[24:]\n",
    "\n",
    "X_test_input = X_test_shift[::48]\n",
    "y_test_input = y_test_shift[::48]\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i in X_test_input:\n",
    "    i_reshaped = i.reshape(1, n_input, n_features)\n",
    "\n",
    "    prediction = model.predict(i_reshaped)\n",
    "\n",
    "     # We want to omit the first 12 hours the predictions i.e., the non-shaded squares in the figure above.\n",
    "    prediction_list = list(prediction[0][24:])\n",
    "    predictions.append(prediction_list)\n",
    "\n",
    "\n",
    "y_test_input_24h = []\n",
    "\n",
    "for i in y_test_input:\n",
    "    # i_reshaped = i.reshape(1, n_input, n_features)\n",
    "    y_test_input_24h.append(i[24:])\n",
    "\n",
    "# Flatten the lists of lists into single lists\n",
    "predictions_flat = [item for sublist in predictions for item in sublist]\n",
    "y_test_flat = [item for sublist in y_test_input_24h for item in sublist]\n",
    "\n",
    "# Calculate MSE\n",
    "test_mse = mean_squared_error(y_test_flat, predictions_flat)\n",
    "\n",
    "# Calculate MAE\n",
    "test_mae = mean_absolute_error(y_test_flat, predictions_flat)\n",
    "\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test MAE: {test_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to change to better visualise the training results\n",
    "\n",
    "# 1 = number of days i.e., 24 hours i.e., 48 time steps\n",
    "\n",
    "start_section = 7\n",
    "end_section = 14\n",
    "diff = end_section-start_section\n",
    "\n",
    "df_ts_test = df_input[n_input+n_split:]\n",
    "\n",
    "X_test_shift = X_test[24:]\n",
    "y_test_shift = y_test[24:]\n",
    "\n",
    "X_test_input = X_test_shift[::48]\n",
    "y_test_input = y_test_shift[::48]\n",
    "\n",
    "\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i in X_test_input[start_section:end_section]:\n",
    "    i_reshaped = i.reshape(1, n_input, n_features)\n",
    "\n",
    "    prediction = model.predict(i_reshaped)\n",
    "\n",
    "     # We want to omit the first 12 hours the predictions i.e., the non-shaded squares in the figure above.\n",
    "    prediction_list = list(prediction[0][24:])\n",
    "    predictions.append(prediction_list)\n",
    "\n",
    "\n",
    "y_test_input_24h = []\n",
    "\n",
    "for i in y_test_input[start_section:end_section]:\n",
    "    # i_reshaped = i.reshape(1, n_input, n_features)\n",
    "    y_test_input_24h.append(i[24:])\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.title(f\"Date Range: {df_ts_test.index[start_section*48+48].strftime('%d/%m/%Y')} - {df_ts_test.index[end_section*48+48].strftime('%d/%m/%Y')}\")\n",
    "\n",
    "# We want to plot the predictions from 00:00 to 00:00 the next day.\n",
    "plt.plot(df_ts_test.index[start_section*48+48: end_section*48+48], np.array(predictions).flatten(), label='Prediction')\n",
    "plt.plot(df_ts_test.index[start_section*48+48: end_section*48+48], np.array(y_test_input_24h).flatten(), label = 'Actual')\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Normalised Net Load (-)')\n",
    "\n",
    "\n",
    "# Generate vertical lines for when we predict\n",
    "xcoords_DAM = df_ts_test.index[start_section*48+72: (end_section)*48+72][::48]\n",
    "\n",
    "for i, xc in enumerate(xcoords_DAM):\n",
    "    if i == 0:  # First item gets the label\n",
    "        plt.axvline(x=xc, color='red', linestyle='--', alpha=0.5, label='DAM Gate Closure')\n",
    "    else:  # Subsequent items do not\n",
    "        plt.axvline(x=xc, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "\n",
    "\n",
    "# Generate vertical lines for DAM closure\n",
    "xcoords_pred = df_ts_test.index[start_section*48+48: (end_section+1)*48+48][::48]\n",
    "\n",
    "for i, xc in enumerate(xcoords_pred):\n",
    "    if i == 0:  # First item gets the label\n",
    "        plt.axvline(x=xc, color='purple', linestyle='--', alpha=0.3, label='Energy Delivery Prediction')\n",
    "    else:  # Subsequent items do not\n",
    "        plt.axvline(x=xc, color='purple', linestyle='--', alpha=0.3)\n",
    "\n",
    "\n",
    "plt.legend(bbox_to_anchor=(0.5, 1.15), loc='center', borderaxespad=0., ncol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "build_model(kt.HyperParameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtuner = kt.RandomSearch(hypermodel=build_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=5,\n",
    "    seed=12345,\n",
    "    overwrite=True,\n",
    "    directory=\"my_dir\",\n",
    "    project_name=\"diab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtuner.search(X_train,\n",
    "              y_train,\n",
    "              epochs=10,\n",
    "              validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model = rtuner.get_best_models(num_models=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtuner.get_best_hyperparameters().get_config()['values']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
